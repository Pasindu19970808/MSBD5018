{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK tutorial\n",
    "(From https://www.nltk.org/)\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "We'll talk about the following sections in this tutorial:\n",
    "\n",
    "1. Tokenizer\n",
    "2. Stemmer\n",
    "3. WordNet\n",
    "4. Tips to the assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.60.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asus\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # to make nltk.tokenizer works\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string.split tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information.']\n",
      "string.split tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly.']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Text mining is to identify useful information.\"\n",
    "text2 = \"Current NLP models isn't able to solve NLU perfectly.\"\n",
    "\n",
    "print(\"string.split tokenizer\", text1.split(\" \"))\n",
    "print(\"string.split tokenizer\", text2.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot deal with punctuations, i.e., full stops and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular expression tokenizer ['', 'ext mining is to identify useful information.']\n",
      "regular expression tokenizer ['', \"urrent NLP models isn't able to solve NLU perfectly.\"]\n"
     ]
    }
   ],
   "source": [
    "import regex # regular expression\n",
    "print(\"regular expression tokenizer\", regex.split(\"^[A-Za-z\\s\\.]\", text1))\n",
    "print(\"regular expression tokenizer\", regex.split(\"^[A-Za-z\\s\\.]\", text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the `string.split` function can not deal with punctuations\n",
    "- Simple regular expression can deal with most punctuations but may fail in the cases of \"isn't, wasn't, can't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "['Current', 'NLP', 'models', 'is', \"n't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text1))\n",
    "print(tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', '(', 'or', ',', 'NLP', ')', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '...']\n"
     ]
    }
   ],
   "source": [
    "# Other examples:\n",
    "# 1. Possessive cases: Apostrophe (isn't, I've, ...)\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect.\")\n",
    "print(tokens)\n",
    "# 2. Parentheses\n",
    "tokens = tokenize(\"Bob's text mining skills (or, NLP) are perfect.\")\n",
    "print(tokens)\n",
    "# 3. ellipsis\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect...\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming and lemmatization\n",
    "\n",
    "(https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "Stemming: chops off the ends of words to acquire the root, and often includes the removal of derivational affixes. \n",
    "\n",
    "e.g., gone -> go, wanted -> want, trees -> tree.\n",
    "\n",
    "Lemmatization: doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n",
    "\n",
    "Differences:\n",
    "The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma (focus on the concrete semantic meaning). \n",
    "\n",
    "E.g.: useful -> use(stemming), useful(lemmatization)\n",
    "\n",
    "PorterStemmer:\n",
    "\n",
    "Rule-based methods. E.g., SSES->SS, IES->I, NOUNS->NOUN. # misses->miss, flies->fli.\n",
    "\n",
    "Doc: https://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = stem(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    return [lm.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = lemmatize(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WordNet\n",
    "\n",
    "https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "- a semantically-oriented dictionary of English,\n",
    "- similar to a traditional thesaurus but with a richer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 synsets\n",
    "\n",
    "A set of one or more **synonyms** that are interchangeable in some context without changing the truth value of the proposition in which they are embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('pursuit.n.01'),\n",
       " Synset('chase.n.02'),\n",
       " Synset('chase.n.03'),\n",
       " Synset('chase.v.01'),\n",
       " Synset('chase.v.02'),\n",
       " Synset('chase.v.03'),\n",
       " Synset('furrow.v.03')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up a word using synsets(); \n",
    "wn.synsets('chase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog',pos = wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synset \t definition\n",
      "Synset('bank.n.01') \t sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') \t a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') \t a long ridge or pile\n",
      "Synset('bank.n.04') \t an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') \t a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') \t the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') \t a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') \t a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') \t a building in which the business of banking transacted\n",
      "Synset('bank.n.10') \t a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') \t tip laterally\n",
      "Synset('bank.v.02') \t enclose with a bank\n",
      "Synset('bank.v.03') \t do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') \t act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') \t be in the banking business\n",
      "Synset('deposit.v.02') \t put into a bank account\n",
      "Synset('bank.v.07') \t cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') \t have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "print(\"synset\",\"\\t\",\"definition\")\n",
    "for synset in wn.synsets('bank'):\n",
    "    print(synset, '\\t', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function has an optional pos argument which lets you constrain the part of speech of the word:\n",
    "# pos: part-of-speech\n",
    "wn.synsets('bank', pos=wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of Synset('dish.n.01') : a piece of dishware normally used as a container for holding or serving food\n",
      "Definition of Synset('dish.n.02') : a particular item of prepared food\n",
      "Definition of Synset('dish.n.03') : the quantity that a dish will hold\n",
      "Definition of Synset('smasher.n.02') : a very attractive or seductive looking woman\n",
      "Definition of Synset('dish.n.05') : directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
      "Definition of Synset('cup_of_tea.n.01') : an activity that you like or at which you are superior\n",
      "Definition of Synset('serve.v.06') : provide (usually but not necessarily food)\n",
      "Definition of Synset('dish.v.02') : make concave; shape like a dish\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dish'):\n",
    "    print(f\"Definition of {synset} : {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('bowl.n.03.bowl')\n",
      "Lemma('butter_dish.n.01.butter_dish')\n",
      "Lemma('casserole.n.02.casserole')\n",
      "Lemma('coquille.n.02.coquille')\n",
      "Lemma('gravy_boat.n.01.gravy_boat')\n",
      "Lemma('gravy_boat.n.01.gravy_holder')\n",
      "Lemma('gravy_boat.n.01.sauceboat')\n",
      "Lemma('gravy_boat.n.01.boat')\n",
      "Lemma('petri_dish.n.01.Petri_dish')\n",
      "Lemma('ramekin.n.02.ramekin')\n",
      "Lemma('ramekin.n.02.ramequin')\n",
      "Lemma('serving_dish.n.01.serving_dish')\n",
      "Lemma('sugar_bowl.n.01.sugar_bowl')\n",
      "Lemma('watch_glass.n.01.watch_glass')\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synset('dish.n.01').hyponyms():\n",
    "    for lemma in synset.lemmas():\n",
    "        print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ramekin', 'ramequin']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('ramekin.n.02').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the dog barked all night']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_hypernyms',\n",
       " '_definition',\n",
       " '_examples',\n",
       " '_frame_ids',\n",
       " '_hypernyms',\n",
       " '_instance_hypernyms',\n",
       " '_iter_hypernym_lists',\n",
       " '_lemma_names',\n",
       " '_lemma_pointers',\n",
       " '_lemmas',\n",
       " '_lexname',\n",
       " '_max_depth',\n",
       " '_min_depth',\n",
       " '_name',\n",
       " '_needs_root',\n",
       " '_offset',\n",
       " '_pointers',\n",
       " '_pos',\n",
       " '_related',\n",
       " '_shortest_hypernym_paths',\n",
       " '_wordnet_corpus_reader',\n",
       " 'acyclic_tree',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'in_region_domains',\n",
       " 'in_topic_domains',\n",
       " 'in_usage_domains',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'mst',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn.synset('dog.n.01'))\n",
    "# isA: hyponyms, hypernyms\n",
    "# part_of: member_holonyms, substance_holonyms, part_holonyms\n",
    "# being part of: member_meronyms, substance_meronyms, part_meronyms\n",
    "# domains: topic_domains, region_domains, usage_domains\n",
    "# attribute: attributes\n",
    "# entailments: entailments\n",
    "# causes: causes\n",
    "# also_sees: also_sees\n",
    "# verb_groups: verb_groups\n",
    "# similar_to: similar_tos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check more relations in http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "hyponyms: [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# hypernyms: abstraction\n",
    "# hyponyms: instantiation\n",
    "\n",
    "dog = wn.synset('dog.n.01')\n",
    "print(\"hypernyms:\", dog.hypernyms())\n",
    "print(\"hyponyms:\", dog.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('carnivore.n.01')]\n",
      "[Synset('placental.n.01')]\n",
      "[Synset('mammal.n.01')]\n",
      "root hypernyms for dog: [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(dog.hypernyms()[0].hypernyms()) # the hypernym of canine\n",
    "# animals that feeds on flesh\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of carnivore\n",
    "# placental mammals\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of placental\n",
    "# mammals\n",
    "# ...\n",
    "print(\"root hypernyms for dog:\", dog.root_hypernyms())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This locates the lowest single hypernym that is shared by two given words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root hypernyms for cat: [Synset('feline.n.01')]\n",
      "root hypernyms for cat: [Synset('entity.n.01')]\n",
      "the lowest common hypernyms of dog and cat\n",
      "[Synset('carnivore.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# find common hypernyms\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').hypernyms())\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').root_hypernyms())\n",
    "print(\"the lowest common hypernyms of dog and cat\")\n",
    "print(wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "paths = motorcar.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('container.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')],\n",
       " [Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('conveyance.n.03'),\n",
       "  Synset('vehicle.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "corgi = wn.synset('corgi.n.01')\n",
    "bensenji = wn.synset('basenji.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat) # dog <- canine <- carnivore -> feline -> cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corgi.path_similarity(dog) # corgi <- dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canine = wn.synset('canine.n.02')\n",
    "canine.path_similarity(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corgi.path_similarity(bensenji) # bensenji <- dog -> corgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "jump = wn.synset('jump.v.01')\n",
    "run = wn.synset('run.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(slap) # 1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(jump) # 1/6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also check:\n",
    "- wup_similarity : 2 * (depth of common hypernym)/ (depth of syn1 + depth of syn2)\n",
    "- lch_similarity -log(p/2d) where p is the depth of the synset closest to both and d is the deepest synset from the 2 synsets\n",
    "- res_similarity\n",
    "...\n",
    "\n",
    "Find more on https://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Traverse the synsets to build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_graph_hypernyms = {}\n",
    "# or you could use networkx package\n",
    "\n",
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "    for hyp_syn in synset.hypernyms():\n",
    "        wn_graph_hypernyms[synset.name()] = {**wn_graph_hypernyms.get(synset.name(), {}), **{hyp_syn.name():True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_graph_hypernyms['physical_entity.n.01']['entity.n.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tips to the assignments\n",
    "\n",
    "Some corpus in the NLTK.\n",
    "\n",
    "Reference: https://www.nltk.org/book/ch02.html. You could search for `gutenberg` and `brown` for detailed documentations.\n",
    "\n",
    "### 4.1 gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg as gb\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = 'austen-sense.txt'\n",
    "word_list = gb.words(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']', 'CHAPTER', '1', 'The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.', 'Their', 'estate', 'was', 'large', ',', 'and', 'their', 'residence', 'was', 'at', 'Norland', 'Park', ',', 'in', 'the', 'centre', 'of', 'their', 'property', ',', 'where', ',', 'for', 'many', 'generations', ',', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'a', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', '.', 'The', 'late', 'owner', 'of', 'this', 'estate', 'was', 'a', 'single', 'man', ',', 'who', 'lived', 'to', 'a', 'very', 'advanced', 'age', ',', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', ',', 'had', 'a', 'constant', 'companion']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gb.sents(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download(\"brown\")\n",
    "print(brown.categories())\n",
    "\n",
    "romance_word_list = brown.words(categories='romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'neither', 'liked', 'nor', 'disliked', 'the', ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romance_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['naphtha','gas','fuel','trade']\n",
    "fileids = [i for i in reuters.fileids() if set(reuters.categories(i)).isdisjoint(set(cats)) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corn', 'grain', 'rice', 'rubber', 'sugar', 'tin', 'trade']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories(fileids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reuters.fileids(\n",
    "\n",
    "    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 316 matches:\n",
      " to the character of an independent nation seems to have been distinguished by\n",
      "f Heaven can never be expected on a nation that disregards the eternal rules o\n",
      "first , the representatives of this nation , then consisting of little more th\n",
      ", situation , and relations of this nation and country than any which had ever\n",
      ", prosperity , and happiness of the nation I have acquired an habitual attachm\n",
      "an be no spectacle presented by any nation more pleasing , more noble , majest\n",
      "party for its own ends , not of the nation for the national good . If that sol\n",
      "tures and the people throughout the nation . On this subject it might become m\n",
      "if a personal esteem for the French nation , formed in a residence of seven ye\n",
      "f our fellow - citizens by whatever nation , and if success can not be obtaine\n",
      "y , continue His blessing upon this nation and its Government and give it all \n",
      "powers so justly inspire . A rising nation , spread over a wide and fruitful l\n",
      "ing now decided by the voice of the nation , announced according to the rules \n",
      "ars witness to the fact that a just nation is trusted on its word when recours\n",
      "e union of opinion which gives to a nation the blessing of harmony and the ben\n",
      "uil suffrage of a free and virtuous nation , would under any circumstances hav\n",
      "d spirit and united councils of the nation will be safeguards to its honor and\n",
      "iction that the war with a powerful nation , which forms so prominent a featur\n",
      "out breaking down the spirit of the nation , destroying all confidence in itse\n",
      "ed on the military resources of the nation . These resources are amply suffici\n",
      "the war to an honorable issue . Our nation is in number more than half that of\n",
      "ndividually have been happy and the nation prosperous . Under this Constitutio\n",
      "rights , and is able to protect the nation against injustice from foreign powe\n",
      " great agricultural interest of the nation prospers under its protection . Loc\n",
      "ak our Union , and demolish us as a nation . Our distance from Europe and the \n"
     ]
    }
   ],
   "source": [
    "text4.concordance('nation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff0d91300876931207232d01add3156fa7c8214350996c757a3c6cebc4b3b5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
