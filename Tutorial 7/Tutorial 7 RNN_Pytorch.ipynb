{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "1. Text classification\n",
    "2. Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
    "#     Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from utils import *\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df[\"id\"], df[\"text\"], df[\"label\"]\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)[\"label\"]\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"id\", \"label\"]\n",
    "    df.to_csv(file_name, index=False)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    regpat = regex.compile(r\"^[A-Za-z0-9\\s]+\")\n",
    "    tokens = [i for i in tokens if regpat.search(i)]\n",
    "    return tokens\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "\n",
    "    return [ps.stem(token).lower() for token in tokens]    \n",
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
    "    :param max_size: the max size of feature dict, type: int\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n",
    "    else:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"]\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            #here the feature is added only if it is lesser than the max limit and greater than the min limit\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        #gets only the features until max_size\n",
    "        valid_feats = valid_feats[:max_size]\n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_index_vector(feats, feats_dict, max_len):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    :param feats: a list of features, type: list\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    #If we were to pad the shorter sentences, then we can add <pad> to the rest of the positions.\n",
    "    #Then in embedding we can set this as the padding_idx, so that word embeddings are 0 for this position\n",
    "    vector = np.zeros(max_len, dtype=np.int64)\n",
    "    for i, f in enumerate(feats):\n",
    "        #Only takes in 50 features from a sentence at maximum\n",
    "        if i == max_len:\n",
    "            break\n",
    "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
    "        #1 is the position of (<unk>)\n",
    "        f_idx = feats_dict.get(f, 1)\n",
    "        vector[i] = f_idx\n",
    "    return vector\n",
    "def create_one_hot_encode_labels(labels,num_classes):\n",
    "    \"\"\"\n",
    "    :params labels: all the labels to be one hot encoded\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(shape = (labels.shape[0],num_classes))\n",
    "    labels = labels - 1\n",
    "    one_hot_vector[range(len(labels)),labels] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./data/train.csv\"\n",
    "test_file = \"./data/test.csv\"\n",
    "ans_file = \"./data/ans.csv\"\n",
    "pred_file = \"./data/pred.csv\"\n",
    "min_freq = 3\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "train_labels = train_labels - 1\n",
    "test_labels = test_labels - 1\n",
    "# extract features\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "train_stemmed = [lemmatize(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [lemmatize(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_stemmed = [remove_punctuation(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [remove_punctuation(tokens) for tokens in test_stemmed]\n",
    "\n",
    "\n",
    "train_feats = train_stemmed\n",
    "test_feats = test_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 5137\n"
     ]
    }
   ],
   "source": [
    "# build a mapping from features to indices\n",
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats),\n",
    "    min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN architecture\n",
    "\n",
    "In this tutorial, we will try to use the recurrent neural network for text classification.\n",
    "\n",
    "![RNN for Text](rnn_for_text.png)\n",
    "\n",
    "The RNN consists of three parts: (1) the word representation part, (2) the recurrent part, and (3) the fully connected part. The word representation part is the word embedding layer; the recurrent part includes multiple (bi-directional) recurrent layers to memorize and summarize contextualized word features; the fully connected part utilizes a multi-layer perceptron to make predictions.\n",
    "\n",
    "\n",
    "### Formula\n",
    "\n",
    "Input: $[w_1, w_2, \\cdots, w_n]$\n",
    "\n",
    "Model: \n",
    "1. Embedding layer: $[e_1, e_2, \\cdots, e_n]$\n",
    "2. RNN -> $[h_1, h_2, \\cdots, h_n]$\n",
    "3. Retrieve the last hidden state $h_n$ as the output embedding for the whole sentence.\n",
    "\n",
    "Output layer:\n",
    "1. Dense layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5137"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "train_feats_matrix = np.vstack([get_index_vector(feats,feats_dict,max_len) for feats in train_feats])\n",
    "testfeats_matrix = np.vstack([get_index_vector(feats,feats_dict,max_len) for feats in test_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one hot labels\n",
    "# train_labels_one_hot = create_one_hot_encode_labels(train_labels,5)\n",
    "# test_labels_one_hot = create_one_hot_encode_labels(test_labels,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index,:],self.labels[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_ds = sentiDataset(train_feats_matrix[:10,:],train_labels[:10])\n",
    "overfit_dl = DataLoader(overfit_ds,batch_size=2)\n",
    "train_ds = sentiDataset(train_feats_matrix,train_labels)\n",
    "train_dl = DataLoader(train_ds,batch_size=10)\n",
    "test_ds = sentiDataset(testfeats_matrix,test_labels)\n",
    "test_dl = DataLoader(test_ds,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiModel(nn.Module):\n",
    "    #input vector for each document will be a vector of length 50. Each element corresponds to position of word \n",
    "    #in the feature dictionary\n",
    "    def __init__(self,**kwargs):\n",
    "        # vocab_size,embedding_dim,hidden_size,num_rnn_layers = 1,bidirectional = False\n",
    "        super(sentiModel,self).__init__()\n",
    "        self.bidirectional = kwargs.pop('bidirectional',False)\n",
    "        self.hidden_dim = kwargs.pop('hidden_size',50)\n",
    "        self.vocab_size = kwargs.pop('vocab_size',50)\n",
    "        self.num_layers = kwargs.pop('num_rnn_layers',1)\n",
    "        self.embedding_dim = kwargs.pop('embedding_dim',50)\n",
    "        self.num_classes = kwargs.pop('num_classes',None)\n",
    "        self.num_batches = kwargs.pop('num_batches',10)\n",
    "        #input will be a vector of length 50\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=self.vocab_size,embedding_dim=self.embedding_dim)\n",
    "        #output will be a vector of length 50x50 from embedding layer (50 words with each word embedded in a 50 dimension vector)\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(input_size = 50,hidden_size=self.hidden_dim,num_layers = self.num_layers, bidirectional = self.bidirectional,batch_first = True)\n",
    "        #Fully connected layer\n",
    "        if self.bidirectional:\n",
    "            self.fcl = nn.Linear(in_features=self.hidden_dim*2,out_features=self.num_classes)\n",
    "        else:\n",
    "            self.fcl = nn.Linear(in_features=self.hidden_dim,out_features=self.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,X,h0,c0):\n",
    "        #out will be of shape [N,50,50]\n",
    "        self.output_dict = {}\n",
    "        out = self.embedding_layer(X)\n",
    "        lstm_out,hidden = self.lstm(out,(h0,c0))\n",
    "        #Take the final states of the backward and forward run, combine and send to a fully connected layer for classification\n",
    "        if self.bidirectional:\n",
    "            bidrectional_state = hidden[0][0,:,:]\n",
    "            forward_state = hidden[0][1,:,:]\n",
    "            #out will be now a [input_sizex(2*hidden_size)] vector\n",
    "            out = torch.concat((forward_state,bidrectional_state),dim=1)\n",
    "        else:\n",
    "            out = hidden[0][0,:,:]\n",
    "        # for layer in self.lstm_layers:\n",
    "        #     out = self.lstm_layers[layer](out)\n",
    "        out = self.fcl(out)\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.bidirectional:\n",
    "            num_layers = 2*self.num_layers\n",
    "        else:\n",
    "            num_layers = self.num_layers\n",
    "        h0 = torch.zeros((num_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((num_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        return h0,c0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'h0' and 'c0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0ae1999c0b5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_list_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_list_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moverfit_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecay_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\MSc\\MSBD 5018\\MSBD5018\\Tutorial 7\\utils\\train_utils.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[1;34m(num_epochs, model, train_dataloader, validation_dataloader, optimizer, loss_fn, device, epoch_divisor, scheduler, decay_epochs)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecay_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mval_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_validation_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn_val_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mtraining_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mvalidation_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0maccuracy_list_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\MSc\\MSBD 5018\\MSBD5018\\Tutorial 7\\utils\\train_utils.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[1;34m(model, dataloader, device, check_precision)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'h0' and 'c0'"
     ]
    }
   ],
   "source": [
    "config = {'bidirectional':True,'hidden_size':50,'vocab_size':len(feats_dict),'num_rnn_layers':1,'embedding_dim':50,'num_classes':5,'num_batches':2}\n",
    "model = sentiModel(**config)\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\n",
    "train_loss_list,val_loss_list,model,accuracy_list_training,accuracy_list_validation = train_utils.train_epochs(10,model,overfit_dl,test_dl,optimizer,loss_fn,device,2,scheduler = None,decay_epochs = None)\n",
    "\n",
    "\n",
    "# for batch,(X,y) in enumerate(train_dl):\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     print(y)\n",
    "#     h0,c0 = model.init_hidden(10)\n",
    "#     print(h0.shape)\n",
    "#     print(c0.shape)\n",
    "#     out = model(X,h0,c0)\n",
    "#     print(out.shape)\n",
    "#     loss = loss_fn(out,y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 1, 2, 3, 1, 4, 1, 3], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 4, 4, 4, 2, 4, 4, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 50])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = hidden[0]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3580e-02,  1.3755e-01,  9.5061e-02, -6.5775e-02,  1.2049e-01,\n",
       "          1.3848e-01, -8.1869e-02,  3.8778e-02,  4.7551e-02, -1.4510e-02,\n",
       "          1.2054e-01, -1.5620e-01,  7.4520e-02,  1.4701e-01,  1.0223e-01,\n",
       "         -7.5184e-02,  1.5924e-02, -1.5599e-03,  8.1864e-02,  1.0590e-01,\n",
       "         -1.1246e-01,  9.4394e-02,  1.9615e-01, -2.4086e-01, -2.9915e-01,\n",
       "          8.5217e-02,  8.5487e-02,  1.4845e-01,  5.5436e-02, -4.1413e-03,\n",
       "          1.0280e-01,  1.3188e-01, -1.7276e-01,  1.5872e-01,  8.1208e-02,\n",
       "          1.5225e-01, -1.7707e-01, -3.8176e-02,  1.3098e-01,  6.1392e-03,\n",
       "          1.0095e-02, -2.8954e-02,  1.1388e-01, -5.4023e-02,  1.2452e-01,\n",
       "          1.7050e-01, -8.0620e-02, -6.5026e-02,  4.2094e-02,  2.4557e-01,\n",
       "         -9.2265e-02,  1.2225e-01, -1.0339e-01, -1.8640e-01, -9.5648e-02,\n",
       "         -2.5018e-02,  3.3677e-01,  8.6114e-02, -1.9252e-02,  3.3813e-03,\n",
       "         -8.3440e-02,  6.5190e-02,  3.0300e-01, -7.3911e-03, -2.3086e-02,\n",
       "         -2.3794e-01,  9.2190e-02, -1.0110e-02, -1.2304e-01, -1.1829e-01,\n",
       "          3.2221e-02,  7.8678e-02,  1.5536e-01,  2.3367e-01,  2.5795e-01,\n",
       "         -1.6256e-01,  4.5335e-02, -1.3767e-01, -1.7546e-01,  1.3192e-01,\n",
       "          9.2230e-02, -4.1446e-03, -2.2718e-02,  8.5341e-02, -1.4947e-01,\n",
       "          1.2370e-01,  1.4468e-01,  3.5084e-02, -1.4062e-01,  8.5971e-02,\n",
       "          2.9322e-02, -9.7578e-02, -1.6485e-01,  3.7639e-02, -1.2298e-01,\n",
       "         -5.1260e-02,  1.4629e-01, -1.3434e-01,  1.0644e-01, -5.2124e-02],\n",
       "        [-1.2650e-01,  4.1571e-02,  7.6256e-02,  3.3739e-02, -7.1018e-02,\n",
       "          1.9350e-01, -2.8229e-02,  4.6198e-02,  1.8554e-01,  3.0521e-02,\n",
       "         -3.0686e-02, -5.1515e-02, -7.4313e-02,  2.6985e-02, -1.4581e-01,\n",
       "         -7.5377e-02,  3.8217e-02, -8.5310e-02,  2.4008e-01,  2.1850e-02,\n",
       "         -1.4678e-01, -1.5924e-02,  1.3324e-01,  7.3261e-02,  1.5075e-01,\n",
       "          1.4432e-01,  3.0281e-01,  1.5958e-01,  5.8974e-02, -4.2342e-02,\n",
       "          2.3493e-01, -1.3722e-02, -2.8810e-01,  3.7159e-02,  2.1374e-01,\n",
       "          1.5148e-01,  2.3197e-01, -2.9866e-01, -3.5240e-02,  1.7007e-01,\n",
       "          1.7693e-01, -7.1905e-02,  1.6162e-01, -2.7834e-01,  2.3226e-01,\n",
       "          1.4713e-01, -1.3335e-01, -3.0331e-02, -1.5980e-01,  1.8597e-01,\n",
       "          2.8380e-01,  5.3389e-02,  1.2881e-01,  1.4911e-02,  1.3560e-01,\n",
       "         -5.7739e-02,  8.0380e-02,  1.8884e-02, -1.3059e-01, -1.1262e-01,\n",
       "          6.2161e-02,  4.6459e-02, -3.1668e-02,  6.4579e-02, -3.0669e-01,\n",
       "         -2.4636e-01, -2.2438e-01,  6.7408e-02, -3.2996e-01,  1.7873e-01,\n",
       "          9.4856e-02,  3.0205e-02, -7.3265e-02,  2.4556e-01, -5.0022e-02,\n",
       "         -1.7346e-01, -4.6099e-02,  7.8981e-02, -8.5256e-02,  6.5567e-02,\n",
       "          1.6278e-01,  9.3994e-03, -1.5522e-01, -5.6779e-02, -3.5168e-01,\n",
       "         -1.5106e-01,  1.0228e-01,  9.5243e-03,  1.7417e-02, -2.9720e-01,\n",
       "         -8.5897e-02, -4.4027e-02,  3.7894e-02, -5.7192e-03, -8.5024e-02,\n",
       "          9.0806e-02, -1.1770e-01, -1.7560e-01, -2.1718e-01,  5.6024e-02],\n",
       "        [-5.9468e-02,  9.3751e-02,  2.1725e-01, -3.4392e-01, -2.2071e-02,\n",
       "         -1.7245e-01, -1.2026e-01, -2.3275e-01,  2.0275e-01, -2.7854e-01,\n",
       "         -4.6896e-02, -1.9719e-01,  4.1313e-02, -8.1292e-02, -8.4792e-02,\n",
       "         -9.1787e-02, -1.2603e-02, -1.2908e-01,  2.5629e-01,  5.4067e-02,\n",
       "          4.5501e-03, -1.9421e-02,  4.8674e-02, -1.0701e-01,  4.3179e-03,\n",
       "         -3.0895e-01, -1.1068e-01,  3.8824e-02,  2.2236e-01, -1.5842e-01,\n",
       "          7.8507e-02, -1.9998e-02,  1.4564e-03,  3.7993e-02,  1.9720e-01,\n",
       "         -8.3001e-02,  6.8553e-02, -2.2907e-01,  7.5609e-02, -1.6757e-01,\n",
       "          6.6071e-02,  2.4534e-02,  1.5963e-01, -1.9958e-01, -1.8251e-01,\n",
       "         -5.6754e-02,  3.6144e-02,  1.3501e-01, -1.8975e-01,  2.1309e-01,\n",
       "         -1.4347e-01,  1.5705e-01, -1.1384e-01,  3.8406e-02, -1.4830e-02,\n",
       "         -2.7697e-02, -4.6171e-02, -5.8403e-02, -1.9310e-01,  1.0617e-01,\n",
       "          1.1790e-02, -8.5643e-02,  9.0801e-02,  1.7530e-02, -1.7309e-01,\n",
       "          2.3528e-01, -9.0510e-03,  1.3818e-01, -2.0624e-02,  8.4818e-02,\n",
       "          7.7930e-02, -1.5300e-01, -3.7281e-02,  1.1227e-01, -6.3991e-02,\n",
       "          4.4717e-02, -8.8413e-03,  1.9114e-01,  9.6497e-02, -3.7337e-02,\n",
       "         -3.0011e-02, -4.1181e-02,  1.0070e-01, -1.7492e-01, -1.2076e-01,\n",
       "         -1.6902e-01,  1.6638e-02, -4.1789e-02,  2.1737e-01, -1.6933e-01,\n",
       "         -2.3567e-01, -1.4208e-02,  3.7829e-02,  1.6431e-01, -2.6492e-01,\n",
       "          8.2278e-02, -1.0874e-01, -5.7047e-02, -1.4322e-01, -1.7745e-01],\n",
       "        [ 8.1147e-03,  7.8325e-03,  1.2598e-01, -2.2752e-01,  1.7944e-01,\n",
       "         -4.8828e-02,  1.9261e-02, -2.0938e-01, -2.3301e-01, -1.7295e-01,\n",
       "          2.0923e-01, -1.9430e-01,  6.5939e-02,  1.2302e-01,  3.3950e-02,\n",
       "         -3.8572e-01, -1.9241e-02, -9.5856e-02,  8.4035e-02,  4.2265e-02,\n",
       "         -1.1212e-01,  2.2325e-01,  1.1181e-01, -2.6190e-01, -2.7407e-01,\n",
       "         -1.6618e-01,  2.1568e-01, -5.6002e-02,  1.2064e-01,  1.8281e-01,\n",
       "         -2.1719e-01, -4.8734e-02,  1.2113e-01,  5.9012e-02,  1.9586e-01,\n",
       "          2.2267e-01,  8.7598e-02, -1.5039e-01, -3.7678e-02,  8.6128e-02,\n",
       "          1.8273e-01,  8.5565e-02, -8.4937e-02, -1.7601e-01, -6.5438e-03,\n",
       "          2.0983e-01,  1.3520e-01,  1.1385e-01,  8.6087e-03,  3.3377e-01,\n",
       "          1.5997e-01, -3.0803e-02, -1.3948e-01,  1.5620e-02, -6.2254e-02,\n",
       "         -6.3740e-02, -2.3142e-01,  1.8216e-02,  1.6498e-01,  4.9684e-01,\n",
       "         -1.0234e-01,  1.1973e-01, -9.3712e-02, -3.7679e-02,  8.4037e-02,\n",
       "         -7.0286e-02, -1.4721e-03, -1.7406e-01, -9.9151e-02, -2.1184e-02,\n",
       "         -1.6839e-01, -6.3227e-02,  1.2748e-01,  1.2877e-01,  2.9416e-01,\n",
       "          1.1736e-01,  7.5242e-02,  9.9652e-03, -2.4320e-01, -1.9646e-02,\n",
       "         -7.6036e-02,  3.0244e-01,  4.3485e-02,  1.3152e-01, -2.6619e-01,\n",
       "         -8.1422e-02, -2.2540e-02,  3.5363e-02, -2.1930e-01, -5.4093e-02,\n",
       "         -4.8691e-02, -2.2031e-01, -2.1999e-01, -1.2989e-01,  1.3183e-01,\n",
       "          3.6948e-03,  1.5520e-01,  7.1069e-02,  3.8233e-02, -4.6746e-02],\n",
       "        [-9.6749e-02,  2.8474e-02,  7.3819e-02, -7.2975e-02,  3.0520e-02,\n",
       "          4.9286e-02, -1.1812e-01,  1.4799e-02, -1.6263e-01, -9.4122e-02,\n",
       "          1.9077e-01, -1.6838e-01,  8.2071e-02, -1.2197e-01, -2.5171e-03,\n",
       "         -1.7823e-01, -1.2951e-01,  1.1978e-02,  6.9810e-03,  1.3798e-01,\n",
       "          1.1082e-01,  8.3277e-02,  1.6421e-01, -1.3644e-01, -2.1076e-01,\n",
       "         -1.4890e-01,  1.5888e-01,  6.0559e-02,  9.7012e-02, -2.1093e-01,\n",
       "         -1.9188e-02,  1.2578e-01,  3.9223e-02,  1.0160e-01,  4.0650e-01,\n",
       "          8.2313e-02,  2.1330e-01, -1.3958e-02,  8.3365e-02, -1.1354e-01,\n",
       "          8.8296e-02, -5.2020e-02,  4.7008e-02, -8.4562e-02, -1.1503e-01,\n",
       "          3.4419e-01,  2.1865e-01, -1.7338e-02, -1.6963e-01,  1.1541e-02,\n",
       "         -8.8251e-02, -1.6634e-01, -1.4208e-02,  1.0382e-01,  4.2798e-02,\n",
       "         -2.3698e-01,  9.5164e-02,  1.7256e-01, -1.4677e-01, -2.0347e-01,\n",
       "         -2.2818e-01, -2.3705e-02,  1.9187e-02, -2.3831e-02, -1.5372e-01,\n",
       "          1.0788e-01, -2.8911e-02,  1.7885e-01, -1.5550e-01, -2.0647e-02,\n",
       "         -3.3033e-02,  6.6811e-03,  3.8261e-03, -1.7309e-02,  4.9741e-02,\n",
       "         -4.3631e-02,  3.8613e-02,  1.2136e-01, -5.5178e-02,  1.2764e-01,\n",
       "          3.5980e-01,  8.0139e-02, -2.0882e-01, -3.5366e-02, -2.6782e-01,\n",
       "         -1.3237e-01,  2.4835e-01, -1.9229e-01, -1.7157e-01,  6.9662e-02,\n",
       "         -1.0095e-01, -1.6269e-01, -7.3124e-02,  1.2885e-01,  9.0440e-02,\n",
       "          1.6496e-01, -2.7844e-02, -2.6130e-02,  7.5179e-02, -2.3249e-02],\n",
       "        [-1.6921e-01, -6.1873e-02,  3.6322e-02, -2.6486e-01, -2.0724e-02,\n",
       "         -4.6360e-03, -4.7478e-02, -1.9839e-01, -1.4100e-02, -1.5754e-01,\n",
       "          1.3592e-01,  1.4657e-01,  3.9086e-02, -7.4170e-02,  7.4329e-02,\n",
       "         -1.1489e-01,  9.8248e-02, -1.0409e-01, -6.8348e-03,  4.2254e-02,\n",
       "          1.6642e-02,  1.8016e-01, -8.4240e-03, -1.1841e-01,  2.7449e-02,\n",
       "         -1.4283e-01,  1.8835e-02, -5.3733e-02,  4.0985e-02,  1.7424e-02,\n",
       "         -3.1199e-02,  9.3910e-02,  1.9757e-01, -4.2830e-02,  1.0469e-01,\n",
       "         -4.5783e-02, -9.0338e-02, -1.2967e-01,  1.8306e-01, -4.0435e-02,\n",
       "         -8.8357e-02,  1.9798e-01,  8.3784e-03, -2.1817e-02, -2.5736e-01,\n",
       "          1.3356e-01,  7.0883e-02,  2.6790e-01, -9.5733e-02,  1.2764e-01,\n",
       "          3.8847e-02,  6.9026e-03,  1.1153e-01,  2.4732e-02,  1.1709e-01,\n",
       "         -2.4780e-01, -9.6458e-02,  1.0381e-01,  8.4425e-02,  5.4486e-03,\n",
       "         -1.0908e-01,  2.1874e-02,  1.4950e-01, -5.6759e-02, -1.8810e-01,\n",
       "          1.9098e-01, -2.0335e-01, -3.8132e-02, -8.2626e-02,  8.6094e-02,\n",
       "          1.0392e-01, -6.0677e-02, -2.5070e-02,  2.8377e-01,  2.0541e-01,\n",
       "          9.4928e-02,  2.9772e-02,  1.3845e-01, -2.0466e-01,  1.2295e-01,\n",
       "          5.1703e-02,  2.2975e-02, -2.3999e-02, -1.6505e-01, -2.1487e-01,\n",
       "         -1.7251e-01,  4.0930e-03, -1.2313e-01,  4.7372e-02, -1.2993e-01,\n",
       "         -2.2191e-01, -7.1588e-02,  7.0149e-02, -1.8977e-01, -2.3216e-02,\n",
       "         -2.8346e-02,  3.8982e-02, -2.3887e-01, -7.0026e-03, -1.2338e-01],\n",
       "        [ 1.0564e-01, -9.0673e-02, -1.8758e-02, -4.0744e-01, -6.6691e-02,\n",
       "          3.0061e-02,  1.4944e-01,  2.5753e-01, -4.1000e-01, -3.4941e-01,\n",
       "          3.2022e-01,  5.7525e-01,  4.3288e-01, -1.3646e-01,  1.0734e-01,\n",
       "         -9.8337e-02, -1.1690e-01, -1.2377e-01, -5.3102e-01,  5.4400e-02,\n",
       "         -2.2814e-01, -4.2003e-01,  1.0272e-01,  1.0740e-01, -5.0000e-01,\n",
       "         -7.8827e-02,  2.4484e-02, -1.5128e-01,  5.7479e-03, -3.1941e-01,\n",
       "         -1.2109e-01, -2.5127e-04,  5.8499e-01, -1.0986e-01,  1.5290e-01,\n",
       "         -4.5784e-02, -1.6929e-01,  2.2533e-01,  3.8692e-01,  1.4487e-01,\n",
       "         -2.8969e-01, -2.5717e-01, -8.4261e-02,  3.1380e-01,  3.1490e-01,\n",
       "         -2.7527e-01, -4.2616e-01, -1.1038e-01,  1.2735e-01,  7.6323e-03,\n",
       "          4.9615e-02, -5.4586e-02,  1.2392e-01, -2.6259e-02, -3.4999e-02,\n",
       "          4.2939e-05, -1.6782e-01,  2.7713e-02, -5.9395e-02,  7.5127e-02,\n",
       "         -1.4862e-01,  3.2866e-02, -7.4238e-02, -5.5397e-02,  1.2174e-01,\n",
       "         -2.9788e-02, -1.3903e-01, -2.4339e-01, -6.6824e-03,  5.5252e-02,\n",
       "          3.6426e-02,  8.8388e-02,  1.6392e-03,  3.0200e-01,  9.8799e-02,\n",
       "          1.0570e-01, -2.2522e-01, -1.8367e-01, -2.2746e-01,  8.1019e-02,\n",
       "          2.0911e-01, -4.6867e-02,  8.1688e-02, -2.2770e-01, -2.0363e-01,\n",
       "         -2.7966e-01,  2.7471e-01,  7.6836e-02, -5.7079e-02,  1.2239e-01,\n",
       "         -5.3492e-03, -3.1568e-02, -3.4230e-02, -9.0789e-02,  7.0662e-02,\n",
       "         -6.7383e-02, -1.0833e-01, -9.2116e-02, -4.4523e-02, -6.3731e-02],\n",
       "        [ 4.0160e-03, -1.8751e-01, -2.9878e-02, -1.4445e-01, -4.1097e-02,\n",
       "          4.4376e-02, -1.7829e-02, -5.2454e-02,  2.0728e-01,  1.7513e-01,\n",
       "         -5.5582e-02,  2.2903e-01,  1.0849e-01,  1.2103e-01,  1.9879e-02,\n",
       "         -6.2475e-02,  3.1182e-01, -4.7037e-02,  1.8115e-01, -9.7950e-03,\n",
       "         -5.9663e-02,  2.5540e-01, -1.3042e-02, -3.2088e-02, -6.3311e-02,\n",
       "          2.3367e-01,  3.0536e-02, -1.6321e-01,  6.7680e-02, -5.7069e-02,\n",
       "         -3.4611e-02,  1.9921e-01,  2.5772e-01, -1.7830e-02, -5.9448e-02,\n",
       "          1.2942e-01, -1.0747e-01,  1.5151e-01,  5.2753e-04,  8.2526e-02,\n",
       "          1.8481e-01,  3.2855e-02, -3.0040e-01,  1.8958e-02, -1.3107e-02,\n",
       "         -6.0063e-02, -1.9336e-03,  3.1404e-02,  1.1209e-01,  1.2210e-01,\n",
       "         -8.2296e-03,  8.8819e-02,  5.9968e-02,  1.2066e-01, -9.2103e-02,\n",
       "         -5.8908e-02,  1.2896e-01,  4.6198e-02,  1.0742e-01,  1.1223e-01,\n",
       "          8.6355e-02, -1.9971e-01,  3.7300e-01, -4.5873e-02,  2.5167e-03,\n",
       "          5.2369e-02,  6.8705e-02, -1.4499e-01, -8.4309e-02,  5.9706e-02,\n",
       "         -1.5023e-02, -4.3257e-02, -7.0387e-02, -2.6813e-02, -7.2442e-02,\n",
       "         -1.3393e-01,  1.9756e-02,  2.5726e-01, -5.3770e-02, -1.7233e-01,\n",
       "          9.1698e-02, -2.4789e-01, -1.7859e-01, -1.7716e-01, -3.2649e-01,\n",
       "          1.3872e-01, -6.2584e-02, -8.1913e-02, -2.4667e-02,  2.2993e-01,\n",
       "         -1.7945e-01,  1.0345e-01,  2.7781e-03,  1.1729e-01,  9.6310e-02,\n",
       "          1.1229e-01,  1.0035e-01, -5.4307e-02, -8.3725e-02, -1.3068e-01],\n",
       "        [ 1.0552e-01, -9.2141e-02, -1.8698e-02, -4.0810e-01, -6.7044e-02,\n",
       "          3.0599e-02,  1.4948e-01,  2.5745e-01, -4.0946e-01, -3.5100e-01,\n",
       "          3.2002e-01,  5.7461e-01,  4.3354e-01, -1.3648e-01,  1.0748e-01,\n",
       "         -9.7749e-02, -1.1685e-01, -1.2471e-01, -5.3417e-01,  5.3669e-02,\n",
       "         -2.2798e-01, -4.2211e-01,  1.0215e-01,  1.0672e-01, -5.0333e-01,\n",
       "         -7.8963e-02,  2.4635e-02, -1.5166e-01,  1.0089e-03, -3.1630e-01,\n",
       "         -1.1689e-01, -1.0585e-03,  5.8645e-01, -1.0860e-01,  1.5279e-01,\n",
       "         -4.6780e-02, -1.6950e-01,  2.2650e-01,  3.8814e-01,  1.4529e-01,\n",
       "         -2.8988e-01, -2.5734e-01, -8.3762e-02,  3.2056e-01,  3.1460e-01,\n",
       "         -2.7529e-01, -4.2675e-01, -1.0895e-01,  1.2725e-01,  7.2576e-03,\n",
       "          1.1239e-01,  6.1383e-02, -6.5435e-02,  3.1654e-02, -1.2683e-01,\n",
       "         -9.2463e-02,  8.3831e-02,  6.7975e-03,  1.0283e-03,  9.4548e-02,\n",
       "         -4.5725e-02, -2.5149e-01,  4.9401e-02,  2.4490e-02, -2.3476e-01,\n",
       "          1.3647e-01, -5.1606e-02,  1.1575e-01,  8.3365e-02, -8.8384e-02,\n",
       "          1.0717e-01, -1.3731e-01, -8.5679e-02,  1.2661e-01,  3.5045e-02,\n",
       "         -8.3912e-02,  7.8399e-02,  1.3845e-01,  2.1485e-02,  3.2242e-02,\n",
       "         -1.4317e-01,  2.7585e-02, -1.5493e-01, -1.8577e-01, -1.7661e-01,\n",
       "         -1.1861e-01, -1.3049e-02, -2.9110e-02, -1.2467e-02, -1.5996e-01,\n",
       "         -1.7082e-01, -6.0721e-02,  1.4551e-01,  3.0060e-01, -8.8051e-02,\n",
       "         -8.7394e-02, -1.1684e-01,  3.4592e-01,  9.2942e-02,  6.3433e-02],\n",
       "        [-2.7301e-02,  1.9864e-02, -1.5023e-01,  2.5194e-01,  1.3311e-01,\n",
       "         -8.7930e-03,  9.6103e-02,  1.9140e-01, -1.0244e-02,  3.7973e-02,\n",
       "         -2.4431e-01,  2.0311e-01, -5.9580e-02, -1.2970e-01, -2.3094e-03,\n",
       "         -6.2398e-02, -4.4351e-02, -1.9241e-01, -1.6576e-02,  1.4137e-01,\n",
       "          5.3350e-02,  3.0621e-02,  1.6558e-03, -1.4406e-01, -1.1038e-01,\n",
       "          1.1484e-01,  6.1655e-02, -8.0658e-02, -2.1285e-01,  1.4923e-01,\n",
       "          1.7025e-01,  3.0290e-02,  7.7398e-02,  7.0310e-02, -2.2245e-01,\n",
       "          1.1101e-01,  1.7174e-01,  1.3905e-01,  3.6697e-02,  5.6175e-02,\n",
       "          7.2554e-02, -1.8661e-02, -6.7037e-02,  3.6495e-02,  5.6599e-02,\n",
       "          1.9103e-02,  5.9875e-02,  1.4296e-01,  4.4417e-02, -2.0556e-02,\n",
       "         -6.2470e-02, -1.0710e-01,  4.1485e-02,  2.0741e-02,  1.5338e-01,\n",
       "          5.7916e-02,  1.1568e-01,  2.2831e-02,  3.4437e-02, -1.2451e-01,\n",
       "         -3.4688e-01, -1.9353e-01, -9.7033e-02, -1.7399e-01, -3.2670e-02,\n",
       "          1.2899e-01, -1.3065e-01,  3.1771e-01, -1.5026e-01, -1.1310e-01,\n",
       "         -1.0826e-02, -3.1330e-01,  1.8155e-01,  3.2372e-01,  2.2224e-01,\n",
       "         -5.4637e-02, -7.9743e-02, -3.0931e-02, -2.7049e-01, -7.7552e-02,\n",
       "          2.6268e-01,  1.3032e-01, -1.1751e-01,  6.8087e-03, -5.4792e-02,\n",
       "         -2.3820e-01, -2.7177e-01, -2.3288e-01,  9.5531e-02, -2.8917e-02,\n",
       "         -2.2063e-01, -2.6266e-02, -8.8163e-02,  8.0020e-02, -1.5403e-01,\n",
       "          8.1647e-02, -1.6927e-01, -5.4954e-02,  4.7715e-02,  3.0706e-02]],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((test[0,:,:],test[1,:,:]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4906e-01, -1.4046e-01, -4.5302e-02, -1.9351e-02,  7.6776e-02,\n",
       "          1.2641e-01, -9.2020e-02, -5.4868e-02, -1.3931e-01, -1.8051e-01,\n",
       "          1.4200e-01, -3.7571e-02,  1.5239e-01,  1.8419e-01, -6.1538e-02,\n",
       "         -9.9384e-02,  9.7652e-04,  4.2894e-03, -2.0926e-01,  6.9417e-02,\n",
       "         -1.1370e-01, -8.3564e-02, -2.5141e-02, -1.3878e-02, -2.0062e-01,\n",
       "         -8.5827e-03,  5.9929e-02,  1.4097e-01,  1.4491e-01, -1.7105e-01,\n",
       "          1.0871e-01, -1.0890e-02,  1.9896e-01, -8.0862e-02,  7.6227e-02,\n",
       "          3.4171e-02, -1.7284e-01,  1.5071e-01,  6.8803e-02, -3.0078e-02,\n",
       "          1.3711e-01, -1.0802e-01,  4.4750e-02, -4.2957e-02,  5.1549e-02,\n",
       "         -5.4431e-02, -6.1013e-02, -7.6933e-02,  1.0767e-01,  1.0803e-01,\n",
       "         -9.2265e-02,  1.2225e-01, -1.0339e-01, -1.8640e-01, -9.5648e-02,\n",
       "         -2.5018e-02,  3.3677e-01,  8.6114e-02, -1.9252e-02,  3.3813e-03,\n",
       "         -8.3440e-02,  6.5190e-02,  3.0300e-01, -7.3911e-03, -2.3086e-02,\n",
       "         -2.3794e-01,  9.2190e-02, -1.0110e-02, -1.2304e-01, -1.1829e-01,\n",
       "          3.2221e-02,  7.8678e-02,  1.5536e-01,  2.3367e-01,  2.5795e-01,\n",
       "         -1.6256e-01,  4.5335e-02, -1.3767e-01, -1.7546e-01,  1.3192e-01,\n",
       "          9.2230e-02, -4.1446e-03, -2.2718e-02,  8.5341e-02, -1.4947e-01,\n",
       "          1.2370e-01,  1.4468e-01,  3.5084e-02, -1.4062e-01,  8.5971e-02,\n",
       "          2.9322e-02, -9.7578e-02, -1.6485e-01,  3.7639e-02, -1.2298e-01,\n",
       "         -5.1260e-02,  1.4629e-01, -1.3434e-01,  1.0644e-01, -5.2124e-02],\n",
       "        [ 3.1635e-02,  8.5030e-02,  6.3881e-02, -1.9651e-01,  8.0867e-02,\n",
       "         -4.9707e-03, -7.9302e-02, -1.5464e-01, -1.8641e-02, -2.8905e-01,\n",
       "         -1.6142e-02, -2.3492e-02,  1.0385e-01, -1.2078e-02,  2.6151e-02,\n",
       "          2.6213e-02, -2.0836e-01,  1.2734e-01, -2.0469e-01, -6.3078e-02,\n",
       "         -6.6503e-02, -9.5390e-02,  1.8016e-02,  1.6912e-01, -6.1928e-02,\n",
       "         -8.9601e-02,  7.4727e-02, -3.9442e-02,  6.6951e-02,  4.0461e-02,\n",
       "          6.0795e-02, -5.5114e-02, -5.4829e-02,  3.0773e-02,  6.9224e-02,\n",
       "         -1.5241e-01, -1.7514e-01, -7.0617e-02,  7.3939e-02, -8.5208e-02,\n",
       "          2.8179e-02, -2.3018e-01, -2.0869e-02, -8.3126e-02, -2.2672e-02,\n",
       "          2.0818e-02, -1.8542e-01,  7.9262e-02,  8.0314e-03,  2.4254e-01,\n",
       "          2.8380e-01,  5.3389e-02,  1.2881e-01,  1.4911e-02,  1.3560e-01,\n",
       "         -5.7739e-02,  8.0380e-02,  1.8884e-02, -1.3059e-01, -1.1262e-01,\n",
       "          6.2161e-02,  4.6459e-02, -3.1668e-02,  6.4579e-02, -3.0669e-01,\n",
       "         -2.4636e-01, -2.2438e-01,  6.7408e-02, -3.2996e-01,  1.7873e-01,\n",
       "          9.4856e-02,  3.0205e-02, -7.3265e-02,  2.4556e-01, -5.0022e-02,\n",
       "         -1.7346e-01, -4.6099e-02,  7.8981e-02, -8.5256e-02,  6.5567e-02,\n",
       "          1.6278e-01,  9.3994e-03, -1.5522e-01, -5.6779e-02, -3.5168e-01,\n",
       "         -1.5106e-01,  1.0228e-01,  9.5243e-03,  1.7417e-02, -2.9720e-01,\n",
       "         -8.5897e-02, -4.4027e-02,  3.7894e-02, -5.7192e-03, -8.5024e-02,\n",
       "          9.0806e-02, -1.1770e-01, -1.7560e-01, -2.1718e-01,  5.6024e-02],\n",
       "        [-1.3180e-01,  8.2292e-02,  1.9751e-01,  4.1836e-03,  1.3574e-01,\n",
       "          8.2851e-02,  8.4725e-02, -8.3281e-02, -1.6256e-01, -1.6854e-01,\n",
       "         -1.7838e-01, -5.1936e-02, -7.0666e-02,  3.7614e-02,  1.9967e-01,\n",
       "          2.3061e-01, -9.2889e-02,  5.2913e-02,  2.1178e-01,  8.4340e-02,\n",
       "         -1.2629e-01, -8.8529e-03, -1.0383e-02,  2.0022e-01, -1.2469e-01,\n",
       "          8.5828e-03, -1.6750e-01, -5.3288e-02, -2.2277e-01,  1.2967e-01,\n",
       "         -1.3439e-01, -1.3382e-02, -1.3804e-01,  1.8232e-02, -5.0022e-02,\n",
       "         -7.1975e-02, -1.2758e-01, -1.4576e-01,  1.8722e-01, -1.2538e-01,\n",
       "         -9.3503e-02,  5.7781e-02,  1.0760e-01,  5.6661e-02, -1.0509e-01,\n",
       "          1.3055e-01,  1.3914e-01,  1.6081e-01,  5.8169e-02,  2.2907e-02,\n",
       "         -1.4347e-01,  1.5705e-01, -1.1384e-01,  3.8406e-02, -1.4830e-02,\n",
       "         -2.7697e-02, -4.6171e-02, -5.8403e-02, -1.9310e-01,  1.0617e-01,\n",
       "          1.1790e-02, -8.5643e-02,  9.0801e-02,  1.7530e-02, -1.7309e-01,\n",
       "          2.3528e-01, -9.0510e-03,  1.3818e-01, -2.0624e-02,  8.4818e-02,\n",
       "          7.7930e-02, -1.5300e-01, -3.7281e-02,  1.1227e-01, -6.3991e-02,\n",
       "          4.4717e-02, -8.8413e-03,  1.9114e-01,  9.6497e-02, -3.7337e-02,\n",
       "         -3.0011e-02, -4.1181e-02,  1.0070e-01, -1.7492e-01, -1.2076e-01,\n",
       "         -1.6902e-01,  1.6638e-02, -4.1789e-02,  2.1737e-01, -1.6933e-01,\n",
       "         -2.3567e-01, -1.4208e-02,  3.7829e-02,  1.6431e-01, -2.6492e-01,\n",
       "          8.2278e-02, -1.0874e-01, -5.7047e-02, -1.4322e-01, -1.7745e-01],\n",
       "        [ 1.8096e-02,  1.9150e-01,  3.6225e-02, -3.6704e-03,  9.9388e-02,\n",
       "          1.1266e-01, -1.5916e-01,  5.6255e-02,  1.3273e-01,  6.1046e-02,\n",
       "          8.4454e-02, -3.9129e-02, -3.0197e-02, -1.1982e-01, -2.4043e-01,\n",
       "          5.1885e-02,  1.6429e-01, -2.6939e-03,  7.7293e-02, -1.4661e-01,\n",
       "          3.4736e-02,  1.7819e-02, -1.9468e-01,  1.1694e-01,  1.7402e-02,\n",
       "          2.4965e-01, -1.5811e-01,  1.1443e-01, -1.0929e-01,  1.2151e-01,\n",
       "          1.5557e-01, -8.2318e-02, -3.4851e-01,  8.1160e-02,  7.7337e-02,\n",
       "          1.4958e-01,  1.4898e-01,  1.4026e-01,  1.4083e-01,  2.5315e-02,\n",
       "         -1.1642e-01, -7.3640e-02,  1.1150e-01,  2.5663e-02, -2.2675e-01,\n",
       "         -2.5510e-02, -8.5491e-02, -1.8495e-01,  1.5796e-02,  3.5727e-02,\n",
       "          1.5997e-01, -3.0803e-02, -1.3948e-01,  1.5620e-02, -6.2254e-02,\n",
       "         -6.3740e-02, -2.3142e-01,  1.8216e-02,  1.6498e-01,  4.9684e-01,\n",
       "         -1.0234e-01,  1.1973e-01, -9.3712e-02, -3.7679e-02,  8.4037e-02,\n",
       "         -7.0286e-02, -1.4721e-03, -1.7406e-01, -9.9151e-02, -2.1184e-02,\n",
       "         -1.6839e-01, -6.3227e-02,  1.2748e-01,  1.2877e-01,  2.9416e-01,\n",
       "          1.1736e-01,  7.5242e-02,  9.9652e-03, -2.4320e-01, -1.9646e-02,\n",
       "         -7.6036e-02,  3.0244e-01,  4.3485e-02,  1.3152e-01, -2.6619e-01,\n",
       "         -8.1422e-02, -2.2540e-02,  3.5363e-02, -2.1930e-01, -5.4093e-02,\n",
       "         -4.8691e-02, -2.2031e-01, -2.1999e-01, -1.2989e-01,  1.3183e-01,\n",
       "          3.6948e-03,  1.5520e-01,  7.1069e-02,  3.8233e-02, -4.6746e-02],\n",
       "        [ 1.3631e-01,  2.7275e-02, -2.2347e-01,  3.8757e-03, -6.1484e-03,\n",
       "          2.6391e-03, -5.3414e-02, -3.0778e-02, -1.1790e-01, -5.0657e-02,\n",
       "          5.3527e-02,  3.4847e-02,  7.7477e-02, -2.0610e-01, -2.3997e-02,\n",
       "         -5.0015e-02,  1.3754e-02, -1.5998e-01, -3.6961e-02, -7.6182e-02,\n",
       "          1.3486e-01, -5.2915e-02,  1.4616e-01,  1.8399e-01, -2.1083e-01,\n",
       "         -3.2322e-02,  2.5119e-03, -3.6735e-02,  2.3673e-02, -8.9732e-03,\n",
       "          9.0468e-02, -5.0312e-02, -1.9513e-02, -9.8332e-02,  5.6975e-02,\n",
       "          6.9922e-02,  6.6923e-02,  2.8753e-02,  3.7553e-02, -4.6238e-02,\n",
       "         -4.8698e-02, -9.6375e-02,  1.3525e-02, -2.9002e-02, -2.3036e-01,\n",
       "         -4.0345e-02,  2.1005e-02, -5.9612e-02, -8.3953e-02,  3.8539e-02,\n",
       "         -8.8251e-02, -1.6634e-01, -1.4208e-02,  1.0382e-01,  4.2798e-02,\n",
       "         -2.3698e-01,  9.5164e-02,  1.7256e-01, -1.4677e-01, -2.0347e-01,\n",
       "         -2.2818e-01, -2.3705e-02,  1.9187e-02, -2.3831e-02, -1.5372e-01,\n",
       "          1.0788e-01, -2.8911e-02,  1.7885e-01, -1.5550e-01, -2.0647e-02,\n",
       "         -3.3033e-02,  6.6811e-03,  3.8261e-03, -1.7309e-02,  4.9741e-02,\n",
       "         -4.3631e-02,  3.8613e-02,  1.2136e-01, -5.5178e-02,  1.2764e-01,\n",
       "          3.5980e-01,  8.0139e-02, -2.0882e-01, -3.5366e-02, -2.6782e-01,\n",
       "         -1.3237e-01,  2.4835e-01, -1.9229e-01, -1.7157e-01,  6.9662e-02,\n",
       "         -1.0095e-01, -1.6269e-01, -7.3124e-02,  1.2885e-01,  9.0440e-02,\n",
       "          1.6496e-01, -2.7844e-02, -2.6130e-02,  7.5179e-02, -2.3249e-02],\n",
       "        [ 4.6396e-02, -9.3603e-02,  3.4652e-02, -5.9231e-02,  4.7924e-02,\n",
       "         -6.9249e-02, -4.4556e-02, -1.5411e-01, -7.5230e-02, -2.4264e-01,\n",
       "         -6.0552e-02, -3.2868e-02,  7.5420e-02,  5.2285e-02, -3.7968e-02,\n",
       "          2.6077e-01, -1.3849e-01,  5.6799e-02,  1.0638e-01,  1.0530e-01,\n",
       "         -6.4148e-02,  8.6135e-02,  4.7754e-02,  9.0225e-02, -1.4929e-01,\n",
       "          1.4055e-01,  1.0079e-01,  5.6135e-02,  7.2845e-02, -9.1602e-02,\n",
       "          2.4085e-01, -8.0700e-02,  3.4815e-02,  5.6310e-02, -1.1857e-01,\n",
       "          6.6708e-02,  4.4680e-02, -1.5951e-02, -1.1654e-01, -7.2014e-02,\n",
       "          1.7870e-02,  8.5865e-03, -2.2571e-01,  5.0563e-02, -7.0871e-02,\n",
       "         -2.1910e-02,  1.4921e-01,  4.7965e-02,  1.6497e-02, -2.9344e-02,\n",
       "          3.8847e-02,  6.9026e-03,  1.1153e-01,  2.4732e-02,  1.1709e-01,\n",
       "         -2.4780e-01, -9.6458e-02,  1.0381e-01,  8.4425e-02,  5.4486e-03,\n",
       "         -1.0908e-01,  2.1874e-02,  1.4950e-01, -5.6759e-02, -1.8810e-01,\n",
       "          1.9098e-01, -2.0335e-01, -3.8132e-02, -8.2626e-02,  8.6094e-02,\n",
       "          1.0392e-01, -6.0677e-02, -2.5070e-02,  2.8377e-01,  2.0541e-01,\n",
       "          9.4928e-02,  2.9772e-02,  1.3845e-01, -2.0466e-01,  1.2295e-01,\n",
       "          5.1703e-02,  2.2975e-02, -2.3999e-02, -1.6505e-01, -2.1487e-01,\n",
       "         -1.7251e-01,  4.0930e-03, -1.2313e-01,  4.7372e-02, -1.2993e-01,\n",
       "         -2.2191e-01, -7.1588e-02,  7.0149e-02, -1.8977e-01, -2.3216e-02,\n",
       "         -2.8346e-02,  3.8982e-02, -2.3887e-01, -7.0026e-03, -1.2338e-01],\n",
       "        [-1.0259e-01,  2.4913e-01, -8.7623e-02,  1.0253e-01,  7.4501e-02,\n",
       "         -1.7369e-02, -1.3011e-01,  4.9056e-02, -7.6442e-02, -2.3344e-02,\n",
       "         -5.4348e-02, -5.1968e-02, -1.5895e-02,  2.3852e-02, -4.6800e-03,\n",
       "          1.5252e-01, -8.8147e-02, -5.4985e-02, -9.0463e-02, -1.4395e-02,\n",
       "          1.2645e-01,  8.5335e-02,  1.1125e-01,  3.0916e-02, -6.2415e-02,\n",
       "         -1.3748e-01,  9.4793e-02,  5.8724e-02, -1.0022e-01,  1.3106e-01,\n",
       "          8.9558e-02, -4.3361e-02,  1.4544e-01, -3.8014e-02,  6.6541e-03,\n",
       "         -1.1310e-01, -7.9504e-02,  6.2834e-03,  5.4510e-02, -4.1236e-02,\n",
       "         -1.0941e-01,  1.2977e-01,  5.5205e-02,  1.3517e-01,  8.0487e-02,\n",
       "         -6.3593e-02,  8.4870e-02, -1.8363e-01,  2.9627e-02,  4.4724e-02,\n",
       "          4.9615e-02, -5.4586e-02,  1.2392e-01, -2.6259e-02, -3.4999e-02,\n",
       "          4.2939e-05, -1.6782e-01,  2.7713e-02, -5.9395e-02,  7.5127e-02,\n",
       "         -1.4862e-01,  3.2866e-02, -7.4238e-02, -5.5397e-02,  1.2174e-01,\n",
       "         -2.9788e-02, -1.3903e-01, -2.4339e-01, -6.6824e-03,  5.5252e-02,\n",
       "          3.6426e-02,  8.8388e-02,  1.6392e-03,  3.0200e-01,  9.8799e-02,\n",
       "          1.0570e-01, -2.2522e-01, -1.8367e-01, -2.2746e-01,  8.1019e-02,\n",
       "          2.0911e-01, -4.6867e-02,  8.1688e-02, -2.2770e-01, -2.0363e-01,\n",
       "         -2.7966e-01,  2.7471e-01,  7.6836e-02, -5.7079e-02,  1.2239e-01,\n",
       "         -5.3492e-03, -3.1568e-02, -3.4230e-02, -9.0789e-02,  7.0662e-02,\n",
       "         -6.7383e-02, -1.0833e-01, -9.2116e-02, -4.4523e-02, -6.3731e-02],\n",
       "        [-1.5879e-01,  2.8358e-02, -8.2512e-02,  2.0983e-01,  2.0357e-02,\n",
       "          5.0003e-02,  1.5348e-01,  9.0955e-03, -8.6190e-02, -7.0648e-02,\n",
       "         -5.3451e-02, -3.8213e-02,  7.3355e-02,  3.3481e-02, -1.8281e-01,\n",
       "          1.1243e-01, -1.2463e-01, -1.2295e-02,  3.4262e-02,  1.9398e-01,\n",
       "         -1.0347e-01, -2.6212e-01,  8.5642e-02, -1.2700e-01, -2.2276e-02,\n",
       "          2.2745e-01, -6.9651e-02, -3.2024e-02,  7.9386e-02, -2.9952e-01,\n",
       "         -8.4629e-02, -3.1308e-02,  2.2978e-01,  1.4845e-01,  1.2900e-01,\n",
       "          4.4560e-02,  2.5825e-02,  3.0043e-01,  2.4942e-01, -6.2324e-02,\n",
       "          9.5891e-02, -2.6433e-03, -1.2808e-01,  2.7077e-02,  7.2730e-02,\n",
       "          1.3661e-01,  1.2755e-01, -1.4868e-01, -1.2493e-01, -1.0497e-01,\n",
       "         -8.2296e-03,  8.8819e-02,  5.9968e-02,  1.2066e-01, -9.2103e-02,\n",
       "         -5.8908e-02,  1.2896e-01,  4.6198e-02,  1.0742e-01,  1.1223e-01,\n",
       "          8.6355e-02, -1.9971e-01,  3.7300e-01, -4.5873e-02,  2.5167e-03,\n",
       "          5.2369e-02,  6.8705e-02, -1.4499e-01, -8.4309e-02,  5.9706e-02,\n",
       "         -1.5023e-02, -4.3257e-02, -7.0387e-02, -2.6813e-02, -7.2442e-02,\n",
       "         -1.3393e-01,  1.9756e-02,  2.5726e-01, -5.3770e-02, -1.7233e-01,\n",
       "          9.1698e-02, -2.4789e-01, -1.7859e-01, -1.7716e-01, -3.2649e-01,\n",
       "          1.3872e-01, -6.2584e-02, -8.1913e-02, -2.4667e-02,  2.2993e-01,\n",
       "         -1.7945e-01,  1.0345e-01,  2.7781e-03,  1.1729e-01,  9.6310e-02,\n",
       "          1.1229e-01,  1.0035e-01, -5.4307e-02, -8.3725e-02, -1.3068e-01],\n",
       "        [-1.4300e-01,  1.4941e-01, -2.1299e-02,  1.2509e-03,  3.0805e-03,\n",
       "          4.2574e-02, -1.1269e-01,  3.3801e-02,  4.4373e-02, -2.7247e-01,\n",
       "          3.6365e-02,  1.2723e-01, -5.0425e-02, -9.8817e-02, -1.9965e-03,\n",
       "          1.1206e-01, -9.9467e-02, -2.1864e-01,  7.7453e-02,  7.9442e-02,\n",
       "          7.6452e-02,  1.3941e-02,  7.5930e-02,  1.4320e-01, -1.4727e-01,\n",
       "         -2.0517e-01, -1.0587e-01,  5.0300e-02, -1.0245e-01,  3.6031e-02,\n",
       "          4.7626e-02, -5.7243e-02, -3.2043e-02, -6.1778e-03,  5.2260e-02,\n",
       "         -7.2251e-02, -2.8376e-02, -9.3495e-02,  1.0489e-01, -2.6250e-01,\n",
       "         -4.6775e-02, -7.4492e-02,  6.2094e-02, -2.7583e-01, -4.1623e-03,\n",
       "          1.2248e-01,  5.6312e-02,  1.7688e-02, -3.1150e-02, -5.3856e-02,\n",
       "          1.1239e-01,  6.1383e-02, -6.5435e-02,  3.1654e-02, -1.2683e-01,\n",
       "         -9.2463e-02,  8.3831e-02,  6.7975e-03,  1.0283e-03,  9.4548e-02,\n",
       "         -4.5725e-02, -2.5149e-01,  4.9401e-02,  2.4490e-02, -2.3476e-01,\n",
       "          1.3647e-01, -5.1606e-02,  1.1575e-01,  8.3365e-02, -8.8384e-02,\n",
       "          1.0717e-01, -1.3731e-01, -8.5679e-02,  1.2661e-01,  3.5045e-02,\n",
       "         -8.3912e-02,  7.8399e-02,  1.3845e-01,  2.1485e-02,  3.2242e-02,\n",
       "         -1.4317e-01,  2.7585e-02, -1.5493e-01, -1.8577e-01, -1.7661e-01,\n",
       "         -1.1861e-01, -1.3049e-02, -2.9110e-02, -1.2467e-02, -1.5996e-01,\n",
       "         -1.7082e-01, -6.0721e-02,  1.4551e-01,  3.0060e-01, -8.8051e-02,\n",
       "         -8.7394e-02, -1.1684e-01,  3.4592e-01,  9.2942e-02,  6.3433e-02],\n",
       "        [ 1.0851e-01,  6.6911e-02, -2.2349e-02,  6.7470e-02,  5.5634e-02,\n",
       "          1.4558e-01,  1.1841e-01,  8.5556e-02, -1.0908e-01, -2.8310e-02,\n",
       "         -2.9015e-02, -2.0413e-02, -1.7703e-01,  2.1259e-01,  1.6675e-01,\n",
       "          2.3820e-01, -1.4641e-01,  8.2616e-02,  1.5702e-01, -5.1174e-03,\n",
       "         -1.3955e-01,  5.3167e-03,  8.7501e-02, -3.3902e-02, -2.1644e-01,\n",
       "          1.2398e-01,  7.8036e-02,  1.0498e-01, -1.0090e-01,  9.2787e-02,\n",
       "          1.8861e-01,  3.0440e-02, -8.4458e-02,  1.0583e-01, -3.4286e-02,\n",
       "          4.4412e-02,  3.6858e-02, -1.0138e-01,  5.5545e-02, -1.7865e-01,\n",
       "         -5.3556e-02,  1.4715e-02,  7.0994e-02, -1.6011e-01,  5.1838e-02,\n",
       "          1.2954e-01, -8.6876e-03,  5.3176e-02,  8.4000e-03, -1.2677e-01,\n",
       "         -6.2470e-02, -1.0710e-01,  4.1485e-02,  2.0741e-02,  1.5338e-01,\n",
       "          5.7916e-02,  1.1568e-01,  2.2831e-02,  3.4437e-02, -1.2451e-01,\n",
       "         -3.4688e-01, -1.9353e-01, -9.7033e-02, -1.7399e-01, -3.2670e-02,\n",
       "          1.2899e-01, -1.3065e-01,  3.1771e-01, -1.5026e-01, -1.1310e-01,\n",
       "         -1.0826e-02, -3.1330e-01,  1.8155e-01,  3.2372e-01,  2.2224e-01,\n",
       "         -5.4637e-02, -7.9743e-02, -3.0931e-02, -2.7049e-01, -7.7552e-02,\n",
       "          2.6268e-01,  1.3032e-01, -1.1751e-01,  6.8087e-03, -5.4792e-02,\n",
       "         -2.3820e-01, -2.7177e-01, -2.3288e-01,  9.5531e-02, -2.8917e-02,\n",
       "         -2.2063e-01, -2.6266e-02, -8.8163e-02,  8.0020e-02, -1.5403e-01,\n",
       "          8.1647e-02, -1.6927e-01, -5.4954e-02,  4.7715e-02,  3.0706e-02]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3580e-02,  1.3755e-01,  9.5061e-02, -6.5775e-02,  1.2049e-01,\n",
       "          1.3848e-01, -8.1869e-02,  3.8778e-02,  4.7551e-02, -1.4510e-02,\n",
       "          1.2054e-01, -1.5620e-01,  7.4520e-02,  1.4701e-01,  1.0223e-01,\n",
       "         -7.5184e-02,  1.5924e-02, -1.5599e-03,  8.1864e-02,  1.0590e-01,\n",
       "         -1.1246e-01,  9.4394e-02,  1.9615e-01, -2.4086e-01, -2.9915e-01,\n",
       "          8.5217e-02,  8.5487e-02,  1.4845e-01,  5.5436e-02, -4.1413e-03,\n",
       "          1.0280e-01,  1.3188e-01, -1.7276e-01,  1.5872e-01,  8.1208e-02,\n",
       "          1.5225e-01, -1.7707e-01, -3.8176e-02,  1.3098e-01,  6.1392e-03,\n",
       "          1.0095e-02, -2.8954e-02,  1.1388e-01, -5.4023e-02,  1.2452e-01,\n",
       "          1.7050e-01, -8.0620e-02, -6.5026e-02,  4.2094e-02,  2.4557e-01,\n",
       "         -9.7406e-02, -1.2245e-01, -9.2123e-02,  6.6646e-02,  5.2326e-02,\n",
       "          2.1714e-01,  2.6614e-01, -5.6376e-02, -3.8756e-02, -7.0547e-03,\n",
       "         -1.8122e-01, -1.8313e-01,  1.9253e-02, -1.2013e-01,  1.0899e-01,\n",
       "         -1.3712e-02, -1.4755e-01, -1.3472e-01, -3.2839e-01, -1.2617e-01,\n",
       "         -8.8689e-02, -1.0044e-01, -1.5693e-02,  1.1202e-01, -1.8994e-01,\n",
       "          6.3103e-02,  2.7498e-02,  1.7386e-01,  1.2647e-01,  9.3071e-02,\n",
       "          1.3782e-01,  4.8295e-02, -6.9484e-02, -5.5964e-02, -8.7333e-02,\n",
       "          7.7500e-02,  1.5731e-01,  4.4192e-02, -1.7236e-01, -8.6130e-02,\n",
       "          2.4684e-02,  8.6714e-02, -6.0243e-02,  2.4754e-02, -1.6378e-01,\n",
       "          5.8201e-05,  2.4459e-01,  6.4637e-02,  5.7475e-02,  6.3226e-02],\n",
       "        [-1.2650e-01,  4.1571e-02,  7.6256e-02,  3.3739e-02, -7.1018e-02,\n",
       "          1.9350e-01, -2.8229e-02,  4.6198e-02,  1.8554e-01,  3.0521e-02,\n",
       "         -3.0686e-02, -5.1515e-02, -7.4313e-02,  2.6985e-02, -1.4581e-01,\n",
       "         -7.5377e-02,  3.8217e-02, -8.5310e-02,  2.4008e-01,  2.1850e-02,\n",
       "         -1.4678e-01, -1.5924e-02,  1.3324e-01,  7.3261e-02,  1.5075e-01,\n",
       "          1.4432e-01,  3.0281e-01,  1.5958e-01,  5.8974e-02, -4.2342e-02,\n",
       "          2.3493e-01, -1.3722e-02, -2.8810e-01,  3.7159e-02,  2.1374e-01,\n",
       "          1.5148e-01,  2.3197e-01, -2.9866e-01, -3.5240e-02,  1.7007e-01,\n",
       "          1.7693e-01, -7.1905e-02,  1.6162e-01, -2.7834e-01,  2.3226e-01,\n",
       "          1.4713e-01, -1.3335e-01, -3.0331e-02, -1.5980e-01,  1.8597e-01,\n",
       "          1.9890e-01, -7.9826e-02,  1.3225e-01,  9.2836e-02,  7.0587e-02,\n",
       "         -1.5536e-02, -1.2288e-01, -7.1607e-02,  1.8019e-01,  2.4494e-01,\n",
       "         -7.9294e-02,  9.8957e-03,  1.1157e-01,  2.1448e-01, -1.1683e-01,\n",
       "         -1.2951e-01,  3.4501e-02, -1.4375e-01, -7.5269e-02,  1.1465e-01,\n",
       "         -7.4176e-02, -4.4343e-02,  2.0752e-01,  3.2559e-02,  1.0449e-01,\n",
       "          7.2457e-03,  1.4781e-01,  1.6016e-01, -1.7346e-01, -1.4451e-01,\n",
       "          1.0422e-01,  1.7484e-01, -7.2712e-02,  2.3832e-01,  1.3374e-04,\n",
       "          7.8244e-02, -2.2819e-01,  4.0450e-02, -2.2888e-01, -6.2840e-03,\n",
       "         -2.0599e-02,  1.0182e-01, -2.4178e-01, -4.1870e-02,  1.1042e-01,\n",
       "         -8.2434e-02,  1.5420e-01, -2.1518e-02,  9.6074e-02, -2.8652e-02],\n",
       "        [-5.9468e-02,  9.3751e-02,  2.1725e-01, -3.4392e-01, -2.2071e-02,\n",
       "         -1.7245e-01, -1.2026e-01, -2.3275e-01,  2.0275e-01, -2.7854e-01,\n",
       "         -4.6896e-02, -1.9719e-01,  4.1313e-02, -8.1292e-02, -8.4792e-02,\n",
       "         -9.1787e-02, -1.2603e-02, -1.2908e-01,  2.5629e-01,  5.4067e-02,\n",
       "          4.5501e-03, -1.9421e-02,  4.8674e-02, -1.0701e-01,  4.3179e-03,\n",
       "         -3.0895e-01, -1.1068e-01,  3.8824e-02,  2.2236e-01, -1.5842e-01,\n",
       "          7.8507e-02, -1.9998e-02,  1.4564e-03,  3.7993e-02,  1.9720e-01,\n",
       "         -8.3001e-02,  6.8553e-02, -2.2907e-01,  7.5609e-02, -1.6757e-01,\n",
       "          6.6071e-02,  2.4534e-02,  1.5963e-01, -1.9958e-01, -1.8251e-01,\n",
       "         -5.6754e-02,  3.6144e-02,  1.3501e-01, -1.8975e-01,  2.1309e-01,\n",
       "          2.1044e-01, -4.0506e-02, -2.6202e-01,  1.1339e-01, -4.5376e-02,\n",
       "         -9.0753e-02, -2.0685e-01,  4.3591e-02, -1.2016e-01,  8.9495e-02,\n",
       "          1.5797e-03,  5.1615e-02,  7.8141e-02, -1.0131e-01, -1.7621e-01,\n",
       "         -9.6312e-02, -1.2459e-01,  8.0483e-02, -5.0882e-02, -1.7266e-01,\n",
       "         -5.5031e-02, -4.3313e-02, -6.6033e-02, -1.9334e-02, -3.3605e-03,\n",
       "          1.9761e-01, -1.6398e-01,  1.2281e-01, -1.0754e-01,  1.3975e-01,\n",
       "         -6.0424e-02,  2.5704e-01,  3.6555e-02,  1.1242e-01, -1.6809e-01,\n",
       "         -1.5861e-01, -6.4638e-02, -8.6747e-03, -1.8755e-01,  1.1420e-02,\n",
       "          2.2933e-01,  3.8410e-02,  9.1346e-02,  1.9610e-01, -1.2991e-01,\n",
       "          4.8304e-02,  8.8080e-02,  3.3226e-01, -1.7371e-01,  8.2013e-02],\n",
       "        [ 8.1147e-03,  7.8325e-03,  1.2598e-01, -2.2752e-01,  1.7944e-01,\n",
       "         -4.8828e-02,  1.9261e-02, -2.0938e-01, -2.3301e-01, -1.7295e-01,\n",
       "          2.0923e-01, -1.9430e-01,  6.5939e-02,  1.2302e-01,  3.3950e-02,\n",
       "         -3.8572e-01, -1.9241e-02, -9.5856e-02,  8.4035e-02,  4.2265e-02,\n",
       "         -1.1212e-01,  2.2325e-01,  1.1181e-01, -2.6190e-01, -2.7407e-01,\n",
       "         -1.6618e-01,  2.1568e-01, -5.6002e-02,  1.2064e-01,  1.8281e-01,\n",
       "         -2.1719e-01, -4.8734e-02,  1.2113e-01,  5.9012e-02,  1.9586e-01,\n",
       "          2.2267e-01,  8.7598e-02, -1.5039e-01, -3.7678e-02,  8.6128e-02,\n",
       "          1.8273e-01,  8.5565e-02, -8.4937e-02, -1.7601e-01, -6.5438e-03,\n",
       "          2.0983e-01,  1.3520e-01,  1.1385e-01,  8.6087e-03,  3.3377e-01,\n",
       "          9.0956e-02,  5.5026e-02,  3.0239e-02, -3.6348e-03, -2.6197e-03,\n",
       "         -1.0811e-01,  1.5292e-01,  8.3065e-02, -3.9731e-02, -1.7408e-01,\n",
       "         -6.5414e-02, -2.1940e-01, -1.5437e-01, -1.9098e-01,  6.9307e-04,\n",
       "          1.8908e-02,  2.1439e-02, -4.3926e-02, -3.2989e-01, -2.1549e-01,\n",
       "          9.0204e-02,  1.2224e-01, -2.1562e-02,  1.3429e-01,  4.8054e-02,\n",
       "         -2.2017e-01, -2.1464e-01,  1.8586e-02,  2.4015e-02, -9.6749e-02,\n",
       "          1.1860e-01, -1.5940e-01, -1.2100e-01, -1.1087e-01, -1.1787e-01,\n",
       "          4.9633e-02,  1.7930e-01,  6.0203e-02,  2.2633e-01,  2.9757e-01,\n",
       "         -2.6112e-01,  1.2217e-01,  1.2424e-01,  6.3789e-02,  4.6750e-02,\n",
       "          1.0767e-01, -5.3145e-03,  7.0543e-02, -9.4440e-02,  5.8559e-04],\n",
       "        [-9.6749e-02,  2.8474e-02,  7.3819e-02, -7.2975e-02,  3.0520e-02,\n",
       "          4.9286e-02, -1.1812e-01,  1.4799e-02, -1.6263e-01, -9.4122e-02,\n",
       "          1.9077e-01, -1.6838e-01,  8.2071e-02, -1.2197e-01, -2.5171e-03,\n",
       "         -1.7823e-01, -1.2951e-01,  1.1978e-02,  6.9810e-03,  1.3798e-01,\n",
       "          1.1082e-01,  8.3277e-02,  1.6421e-01, -1.3644e-01, -2.1076e-01,\n",
       "         -1.4890e-01,  1.5888e-01,  6.0559e-02,  9.7012e-02, -2.1093e-01,\n",
       "         -1.9188e-02,  1.2578e-01,  3.9223e-02,  1.0160e-01,  4.0650e-01,\n",
       "          8.2313e-02,  2.1330e-01, -1.3958e-02,  8.3365e-02, -1.1354e-01,\n",
       "          8.8296e-02, -5.2020e-02,  4.7008e-02, -8.4562e-02, -1.1503e-01,\n",
       "          3.4419e-01,  2.1865e-01, -1.7338e-02, -1.6963e-01,  1.1541e-02,\n",
       "         -3.3268e-02, -1.4013e-01, -2.2884e-01, -1.4008e-01,  6.8354e-02,\n",
       "         -1.4570e-01,  2.8882e-02,  1.0918e-01,  2.2226e-01,  8.8333e-02,\n",
       "         -3.2657e-02, -7.4024e-02,  8.4167e-03, -6.4435e-02, -1.2263e-01,\n",
       "         -5.2225e-02, -1.8870e-02,  5.7687e-02,  2.0002e-02,  2.7461e-02,\n",
       "         -1.0592e-01, -1.0360e-02, -9.0581e-02, -5.1927e-02, -3.0739e-01,\n",
       "         -9.2119e-02,  1.9281e-01,  2.3632e-01,  6.3989e-02,  3.3031e-01,\n",
       "          1.4039e-01, -1.7360e-01, -3.9591e-02, -1.7475e-01, -1.1838e-01,\n",
       "         -3.4596e-02,  1.3029e-01, -9.2879e-02, -1.6201e-01, -2.2071e-01,\n",
       "         -1.2038e-01,  7.3473e-02,  5.9121e-02,  2.0719e-01,  1.0179e-03,\n",
       "          2.0371e-02,  7.9087e-02,  2.1914e-01, -1.3689e-01,  3.4742e-02],\n",
       "        [-1.6921e-01, -6.1873e-02,  3.6322e-02, -2.6486e-01, -2.0724e-02,\n",
       "         -4.6360e-03, -4.7478e-02, -1.9839e-01, -1.4100e-02, -1.5754e-01,\n",
       "          1.3592e-01,  1.4657e-01,  3.9086e-02, -7.4170e-02,  7.4329e-02,\n",
       "         -1.1489e-01,  9.8248e-02, -1.0409e-01, -6.8348e-03,  4.2254e-02,\n",
       "          1.6642e-02,  1.8016e-01, -8.4240e-03, -1.1841e-01,  2.7449e-02,\n",
       "         -1.4283e-01,  1.8835e-02, -5.3733e-02,  4.0985e-02,  1.7424e-02,\n",
       "         -3.1199e-02,  9.3910e-02,  1.9757e-01, -4.2830e-02,  1.0469e-01,\n",
       "         -4.5783e-02, -9.0338e-02, -1.2967e-01,  1.8306e-01, -4.0435e-02,\n",
       "         -8.8357e-02,  1.9798e-01,  8.3784e-03, -2.1817e-02, -2.5736e-01,\n",
       "          1.3356e-01,  7.0883e-02,  2.6790e-01, -9.5733e-02,  1.2764e-01,\n",
       "          1.2522e-02, -8.9277e-02,  1.3030e-02,  1.8520e-01, -1.1330e-01,\n",
       "         -2.2736e-03, -7.5929e-02,  9.2594e-02, -1.9648e-02,  5.9210e-02,\n",
       "         -1.2166e-01,  1.1211e-02,  7.6298e-02, -1.2544e-01,  4.0182e-02,\n",
       "          8.3512e-02,  2.3439e-02, -9.0239e-02, -1.0919e-01, -1.6406e-01,\n",
       "          6.8236e-02, -1.9266e-02, -8.1807e-02, -1.0174e-02,  2.0109e-02,\n",
       "          1.2273e-01, -9.0170e-02, -1.7794e-01, -1.5521e-02,  1.2348e-01,\n",
       "         -1.6380e-03, -2.3103e-01, -7.0377e-02, -1.6967e-01, -6.3988e-02,\n",
       "          3.3909e-02,  6.5686e-02,  1.4041e-01, -1.1320e-01,  9.0730e-02,\n",
       "          2.4606e-02, -1.4647e-01,  1.9114e-01,  9.3653e-02, -7.0799e-03,\n",
       "         -1.2105e-01, -1.5445e-01,  1.4046e-01, -1.6715e-02,  7.5247e-03],\n",
       "        [ 1.0564e-01, -9.0673e-02, -1.8758e-02, -4.0744e-01, -6.6691e-02,\n",
       "          3.0061e-02,  1.4944e-01,  2.5753e-01, -4.1000e-01, -3.4941e-01,\n",
       "          3.2022e-01,  5.7525e-01,  4.3288e-01, -1.3646e-01,  1.0734e-01,\n",
       "         -9.8337e-02, -1.1690e-01, -1.2377e-01, -5.3102e-01,  5.4400e-02,\n",
       "         -2.2814e-01, -4.2003e-01,  1.0272e-01,  1.0740e-01, -5.0000e-01,\n",
       "         -7.8827e-02,  2.4484e-02, -1.5128e-01,  5.7479e-03, -3.1941e-01,\n",
       "         -1.2109e-01, -2.5127e-04,  5.8499e-01, -1.0986e-01,  1.5290e-01,\n",
       "         -4.5784e-02, -1.6929e-01,  2.2533e-01,  3.8692e-01,  1.4487e-01,\n",
       "         -2.8969e-01, -2.5717e-01, -8.4261e-02,  3.1380e-01,  3.1490e-01,\n",
       "         -2.7527e-01, -4.2616e-01, -1.1038e-01,  1.2735e-01,  7.6323e-03,\n",
       "         -1.2762e-01, -1.1098e-02,  2.0966e-02, -7.2486e-02, -1.2199e-01,\n",
       "         -6.0486e-02,  1.7818e-01, -4.4563e-02,  1.8561e-02, -2.0252e-02,\n",
       "         -6.9648e-03,  1.2993e-01,  1.5054e-01,  2.0514e-01, -5.6853e-02,\n",
       "          2.2575e-02, -3.6572e-02,  4.1842e-02, -2.2063e-01, -1.1006e-01,\n",
       "          5.4733e-03,  1.3023e-01, -8.3104e-02,  6.2861e-02, -1.6504e-02,\n",
       "          6.1879e-02, -1.3494e-02, -1.1438e-02,  5.8106e-02,  1.6148e-02,\n",
       "          8.6002e-02, -9.9090e-02, -3.7574e-02,  5.3659e-02, -1.1852e-01,\n",
       "          2.2447e-01,  2.8072e-02, -4.5404e-02, -4.6801e-02,  5.6474e-02,\n",
       "         -1.6744e-01, -3.0374e-02,  7.0333e-02, -3.2000e-02,  1.8217e-01,\n",
       "          1.3610e-02,  3.3813e-02, -4.6364e-02,  3.1977e-02,  5.5033e-02],\n",
       "        [ 4.0160e-03, -1.8751e-01, -2.9878e-02, -1.4445e-01, -4.1097e-02,\n",
       "          4.4376e-02, -1.7829e-02, -5.2454e-02,  2.0728e-01,  1.7513e-01,\n",
       "         -5.5582e-02,  2.2903e-01,  1.0849e-01,  1.2103e-01,  1.9879e-02,\n",
       "         -6.2475e-02,  3.1182e-01, -4.7037e-02,  1.8115e-01, -9.7950e-03,\n",
       "         -5.9663e-02,  2.5540e-01, -1.3042e-02, -3.2088e-02, -6.3311e-02,\n",
       "          2.3367e-01,  3.0536e-02, -1.6321e-01,  6.7680e-02, -5.7069e-02,\n",
       "         -3.4611e-02,  1.9921e-01,  2.5772e-01, -1.7830e-02, -5.9448e-02,\n",
       "          1.2942e-01, -1.0747e-01,  1.5151e-01,  5.2753e-04,  8.2526e-02,\n",
       "          1.8481e-01,  3.2855e-02, -3.0040e-01,  1.8958e-02, -1.3107e-02,\n",
       "         -6.0063e-02, -1.9336e-03,  3.1404e-02,  1.1209e-01,  1.2210e-01,\n",
       "          1.0052e-01,  5.5799e-03, -4.6030e-02,  1.8831e-02, -2.6077e-02,\n",
       "         -7.5046e-02,  1.6121e-01,  6.3722e-02, -1.7626e-01, -1.4794e-01,\n",
       "          7.6578e-02, -1.1230e-03,  4.6285e-02, -6.7544e-02,  6.2696e-02,\n",
       "          4.7695e-02, -4.4310e-02,  1.1157e-02, -8.0681e-02, -7.0166e-02,\n",
       "         -6.2902e-02,  1.0799e-01,  9.0263e-02,  4.9221e-02,  6.4361e-02,\n",
       "         -6.5580e-02,  2.6504e-02, -2.1204e-02, -8.7035e-02, -5.5921e-02,\n",
       "          5.8686e-02,  2.6346e-01,  1.1526e-01,  6.3704e-03, -5.1571e-02,\n",
       "          1.8440e-01,  1.1236e-01,  3.4863e-01, -2.5619e-02,  7.9301e-02,\n",
       "          7.0059e-02,  4.9384e-02, -1.9742e-01, -5.7193e-02, -1.7397e-02,\n",
       "         -8.9584e-02,  7.4317e-02, -8.0192e-02,  2.5061e-01, -1.0374e-01],\n",
       "        [ 1.0552e-01, -9.2141e-02, -1.8698e-02, -4.0810e-01, -6.7044e-02,\n",
       "          3.0599e-02,  1.4948e-01,  2.5745e-01, -4.0946e-01, -3.5100e-01,\n",
       "          3.2002e-01,  5.7461e-01,  4.3354e-01, -1.3648e-01,  1.0748e-01,\n",
       "         -9.7749e-02, -1.1685e-01, -1.2471e-01, -5.3417e-01,  5.3669e-02,\n",
       "         -2.2798e-01, -4.2211e-01,  1.0215e-01,  1.0672e-01, -5.0333e-01,\n",
       "         -7.8963e-02,  2.4635e-02, -1.5166e-01,  1.0089e-03, -3.1630e-01,\n",
       "         -1.1689e-01, -1.0585e-03,  5.8645e-01, -1.0860e-01,  1.5279e-01,\n",
       "         -4.6780e-02, -1.6950e-01,  2.2650e-01,  3.8814e-01,  1.4529e-01,\n",
       "         -2.8988e-01, -2.5734e-01, -8.3762e-02,  3.2056e-01,  3.1460e-01,\n",
       "         -2.7529e-01, -4.2675e-01, -1.0895e-01,  1.2725e-01,  7.2576e-03,\n",
       "         -1.2762e-01, -1.1098e-02,  2.0966e-02, -7.2486e-02, -1.2199e-01,\n",
       "         -6.0486e-02,  1.7818e-01, -4.4563e-02,  1.8561e-02, -2.0252e-02,\n",
       "         -6.9648e-03,  1.2993e-01,  1.5054e-01,  2.0514e-01, -5.6853e-02,\n",
       "          2.2575e-02, -3.6572e-02,  4.1842e-02, -2.2063e-01, -1.1006e-01,\n",
       "          5.4733e-03,  1.3023e-01, -8.3104e-02,  6.2861e-02, -1.6504e-02,\n",
       "          6.1879e-02, -1.3494e-02, -1.1438e-02,  5.8106e-02,  1.6148e-02,\n",
       "          8.6002e-02, -9.9090e-02, -3.7574e-02,  5.3659e-02, -1.1852e-01,\n",
       "          2.2447e-01,  2.8072e-02, -4.5404e-02, -4.6801e-02,  5.6474e-02,\n",
       "         -1.6744e-01, -3.0374e-02,  7.0333e-02, -3.2000e-02,  1.8217e-01,\n",
       "          1.3610e-02,  3.3813e-02, -4.6364e-02,  3.1977e-02,  5.5033e-02],\n",
       "        [-2.7301e-02,  1.9864e-02, -1.5023e-01,  2.5194e-01,  1.3311e-01,\n",
       "         -8.7930e-03,  9.6103e-02,  1.9140e-01, -1.0244e-02,  3.7973e-02,\n",
       "         -2.4431e-01,  2.0311e-01, -5.9580e-02, -1.2970e-01, -2.3094e-03,\n",
       "         -6.2398e-02, -4.4351e-02, -1.9241e-01, -1.6576e-02,  1.4137e-01,\n",
       "          5.3350e-02,  3.0621e-02,  1.6558e-03, -1.4406e-01, -1.1038e-01,\n",
       "          1.1484e-01,  6.1655e-02, -8.0658e-02, -2.1285e-01,  1.4923e-01,\n",
       "          1.7025e-01,  3.0290e-02,  7.7398e-02,  7.0310e-02, -2.2245e-01,\n",
       "          1.1101e-01,  1.7174e-01,  1.3905e-01,  3.6697e-02,  5.6175e-02,\n",
       "          7.2554e-02, -1.8661e-02, -6.7037e-02,  3.6495e-02,  5.6599e-02,\n",
       "          1.9103e-02,  5.9875e-02,  1.4296e-01,  4.4417e-02, -2.0556e-02,\n",
       "          1.3935e-01, -2.3855e-03,  2.1114e-01,  1.0126e-01,  1.9590e-02,\n",
       "          9.2583e-02,  1.0889e-01,  2.6689e-01, -8.4003e-03, -9.4958e-02,\n",
       "          2.8101e-02, -1.8012e-02,  1.7840e-01,  1.6902e-01,  1.5563e-02,\n",
       "         -1.3155e-02, -3.3992e-02,  5.0671e-02, -1.0108e-01,  8.2242e-02,\n",
       "         -4.9876e-03, -1.4161e-01,  4.0059e-02, -8.8669e-02,  1.2861e-02,\n",
       "          6.5656e-03, -2.2985e-01, -1.0741e-01,  1.1899e-01, -1.2726e-01,\n",
       "          2.2226e-01,  1.0671e-01, -7.0464e-02,  8.0463e-03,  2.0300e-01,\n",
       "          1.6340e-01,  8.6534e-02, -7.8758e-02,  1.7764e-01,  1.1930e-01,\n",
       "         -2.9714e-02,  6.2704e-02,  1.6036e-01, -9.7779e-04,  8.1989e-02,\n",
       "          2.4115e-02,  1.1432e-01,  1.2023e-01,  1.2778e-01,  2.7387e-02]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_layer.weight\n",
      "Embedding(50, 50)\n",
      "lstm.weight_ih_l0\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_hh_l0\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_ih_l0\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_hh_l0\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_ih_l0_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_hh_l0_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_ih_l0_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_hh_l0_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_ih_l1\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_hh_l1\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_ih_l1\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_hh_l1\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_ih_l1_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.weight_hh_l1_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_ih_l1_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "lstm.bias_hh_l1_reverse\n",
      "LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(getattr(model, name.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Im  lolol'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "text = \"Hi I'm , #&&&lolol\"\n",
    "\n",
    "re.sub(r\"[^a-zA-Z0-9\\s]+\",\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Embedding(num_embeddings=len(feats_dict),embedding_dim=50)\n",
    "layer(torch.tensor(train_feats_matrix[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  42,   47, 1109,    5,  210,   12, 2311,    3,   80,   25,   41,\n",
       "        390,    6, 2312,   61,    1,  327,  316, 2585,   22,    2,  384,\n",
       "          8,    2,  418,   27, 2313,    3,   28,   80,   25,  183,    5,\n",
       "          1,   11,    2,    1, 2087,    4,  243,    1,    1,   22,  675,\n",
       "          3, 1145,   61, 2088,   22,  665], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "An RNN Language model is provided here.\n",
    "\n",
    "Input:\n",
    "- word tokens $[w_1, w_2, \\cdots, w_n]$\n",
    "\n",
    "Model\n",
    "- embedding layer: get the representation of all the words as $[e_1, e_2, \\cdots, e_n]$.\n",
    "- RNN: get the hidden representation of the sentence $[h_1, h_2, \\cdots, h_n]$.\n",
    "- Objective: Minimize the log probability of the sentence.\n",
    "\n",
    "\n",
    "**Chain rule:**\n",
    "\n",
    "$P(w_1w_2\\cdots w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1w_2)\\cdots$\n",
    "\n",
    "**Markov approximation**\n",
    "\n",
    "$P(w_1w_2\\cdots w_n) \\approx P(w_1|w_0)P(w_2|w_1)P(w_3|w_2)\\cdots$\n",
    "\n",
    "$P(w_1w_2\\cdots w_n) \\approx \\prod_{i=1}^{n}P(w_i|h_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "(32389, 30)\n",
      "(32389, 30, 1)\n",
      "Vocab Size 9860\n"
     ]
    }
   ],
   "source": [
    "from ptb_loader import load_data\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    \"\"\"\n",
    "    Calculate Perplexity\n",
    "    \"\"\"\n",
    "    def __init__(self, test_data, model):\n",
    "        self.test_data = test_data\n",
    "        self.model = model\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        x_probs = self.model.predict(x)\n",
    "        ppl = self.evaluate_batch_ppl(x_probs,y)\n",
    "        print('\\nValidation Set Perplexity: {0:.2f} \\n'.format(ppl))\n",
    "    @staticmethod\n",
    "    def evaluate_ppl(x, y):\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        y = y.reshape(-1)\n",
    "        return np.exp(np.mean(-np.log(np.diag(x[:, y]))))\n",
    "    def evaluate_batch_ppl(self, x, y):\n",
    "        eval_batch_size = 8\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        y = y.reshape(-1)\n",
    "        ppl = 0.0\n",
    "        for i in range(math.ceil(len(x)/eval_batch_size)):\n",
    "            batch_x = x[i*eval_batch_size:(i+1)*eval_batch_size,:]\n",
    "            batch_y = y[i*eval_batch_size:(i+1)*eval_batch_size]\n",
    "            ppl += np.sum(np.log(np.diag(batch_x[:, batch_y])))\n",
    "        return np.exp(-ppl/x.shape[0])\n",
    "\n",
    "print('Loading data')\n",
    "x_train, y_train, x_valid, y_valid, vocabulary_size, vocab = load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "num_training_data = x_train.shape[0]\n",
    "sequence_length = x_train.shape[1]\n",
    "\n",
    "print('Vocab Size',vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "drop = 0.5\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "embedding_dim = 10\n",
    "\n",
    "# lstm parameters\n",
    "hidden_size = 10\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "# inputs -> [batch_size, sequence_length]\n",
    "\n",
    "emb_layer = Embedding(input_dim=vocabulary_size, \n",
    "                    output_dim=embedding_dim, \n",
    "                    input_length=sequence_length)\n",
    "# emb_layer.trainable = False\n",
    "# if you uncomment this line, the embeddings will be untrainable\n",
    "\n",
    "embedding = emb_layer(inputs)\n",
    "# embedding -> [batch_size, sequence_length, embedding_dim]\n",
    "\n",
    "drop_embed = Dropout(drop)(embedding) \n",
    "# dropout at embedding layer\n",
    "\n",
    "# add a LSTM here, set units=hidden_size, return_sequences=True\n",
    "# Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "lstm_out_1 = LSTM(units=hidden_size, return_sequences=True)(drop_embed)\n",
    "# NER [tag1, tag2, tag3, ...]\n",
    "# output: lstm_out_1 -> [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "\n",
    "# add a TimeDistributed here, set layer = Dense(units=vocabulary_size,activation='softmax')\n",
    "# please read  https://keras.io/layers/wrappers/\n",
    "# output: outputs -> [batch_size, sequence_length, vocabulary_size]\n",
    "outputs = TimeDistributed(Dense(units=vocabulary_size,\n",
    "    activation='softmax'))(lstm_out_1)\n",
    "# [batch_size, sequence_length, output_size]\n",
    "\n",
    "# End of Model Architecture\n",
    "# ----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 10)            98600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 10)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 10)            840       \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 30, 9860)          108460    \n",
      "=================================================================\n",
      "Total params: 207,900\n",
      "Trainable params: 207,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Traning Model...\n",
      "Epoch 1/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 6.5066\n",
      "Validation Set Perplexity: 450.09 \n",
      "\n",
      "4049/4049 [==============================] - 133s 33ms/step - loss: 6.5065\n",
      "Epoch 2/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 6.0937\n",
      "Validation Set Perplexity: 365.04 \n",
      "\n",
      "4049/4049 [==============================] - 118s 29ms/step - loss: 6.0936\n",
      "Epoch 3/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.9352\n",
      "Validation Set Perplexity: 321.49 \n",
      "\n",
      "4049/4049 [==============================] - 111s 27ms/step - loss: 5.9352\n",
      "Epoch 4/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 5.8269\n",
      "Validation Set Perplexity: 291.54 \n",
      "\n",
      "4049/4049 [==============================] - 109s 27ms/step - loss: 5.8269\n",
      "Epoch 5/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.7595\n",
      "Validation Set Perplexity: 276.49 \n",
      "\n",
      "4049/4049 [==============================] - 114s 28ms/step - loss: 5.7596\n",
      "Epoch 6/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.7157\n",
      "Validation Set Perplexity: 267.84 \n",
      "\n",
      "4049/4049 [==============================] - 124s 31ms/step - loss: 5.7157\n",
      "Epoch 7/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.6834\n",
      "Validation Set Perplexity: 260.92 \n",
      "\n",
      "4049/4049 [==============================] - 118s 29ms/step - loss: 5.6835\n",
      "Epoch 8/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 5.6590\n",
      "Validation Set Perplexity: 254.94 \n",
      "\n",
      "4049/4049 [==============================] - 113s 28ms/step - loss: 5.6590\n",
      "Epoch 9/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.6364\n",
      "Validation Set Perplexity: 249.97 \n",
      "\n",
      "4049/4049 [==============================] - 113s 28ms/step - loss: 5.6364\n",
      "Epoch 10/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 5.6174\n",
      "Validation Set Perplexity: 245.53 \n",
      "\n",
      "4049/4049 [==============================] - 116s 29ms/step - loss: 5.6173\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = keras.optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print(\"Traning Model...\")\n",
    "history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[TestCallback((x_valid,y_valid),model=model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [vocab[s] for s in \"i visited the campus last monday\".split()]\n",
    "sent_2 = [vocab[s] for s in \"i visited the campus last pizza\".split()]\n",
    "sent_1_input = np.expand_dims(np.array(sent_1 + [0] * (x_train.shape[1]-len(sent_1))), 0)\n",
    "sent_2_input = np.expand_dims(np.array(sent_2 + [0] * (x_train.shape[1]-len(sent_2))), 0)\n",
    "sent_1_y = np.expand_dims([sent_1[1:]+[sent_1[0]]], -1)\n",
    "sent_2_y = np.expand_dims([sent_2[1:]+[sent_2[0]]], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of sentence 1: 2119.398\n",
      "perplexity of sentence 2: 3703.2559\n"
     ]
    }
   ],
   "source": [
    "print(\"perplexity of sentence 1:\", TestCallback.evaluate_ppl(model.predict(sent_1_input)[:len(sent_1)], \n",
    "                          sent_1_y))\n",
    "print(\"perplexity of sentence 2:\", TestCallback.evaluate_ppl(model.predict(sent_2_input)[:len(sent_1)], \n",
    "                          sent_2_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff0d91300876931207232d01add3156fa7c8214350996c757a3c6cebc4b3b5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
