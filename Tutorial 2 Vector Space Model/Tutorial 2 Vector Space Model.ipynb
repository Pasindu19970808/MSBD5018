{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "\n",
    "Text preprocessing piplines:\n",
    "\n",
    "Text denoising -> text normalization (Tutorial 1) -> text standarization (Tutorial 2)\n",
    "\n",
    "Normalization involves tokenization and lemmatization. However lematization is optional. For topic classification it is not required.\n",
    "\n",
    "In this tutorial we'll learn how to conduct text standarization using vector space model.\n",
    "\n",
    "1. Stop words, ngrams, the whole pipeline of text preprocessing.\n",
    "2. Bag-of-word representation\n",
    "3. Term weighting\n",
    " - Term frequency\n",
    " - Inverse document frequency\n",
    " - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stop words, ngrams, the whole pipeline of text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords's intuition: Not all words are informative\n",
    "- Remove such words to reduce vocabulary size\n",
    "- No universal definition\n",
    "- Risk: break the original meaning and structure of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'few', 'both', 'at', 'couldn', 'have', \"won't\", 'on', 'not', 'hasn', 've', 'over', 'doesn', 'your', \"shan't\", 'me', 'between', 'yours', \"mustn't\", 'very', 'her', \"it's\", 'out', 'are', 'it', \"isn't\", 'our', \"shouldn't\", 'above', 'once', \"weren't\", 'up', 'or', 'do', 'd', 'has', 'y', 'did', 'haven', \"haven't\", 'just', 'his', 'in', 'is', 'what', 'as', 'didn', 'being', 'hadn', 'isn', 'he', 'will', \"should've\", 'll', 'here', 'who', 'we', 'they', 'a', 'off', 'how', 'themselves', 'below', 'o', 'this', 'an', 'should', 'can', 'herself', 'my', 'these', 'shouldn', 'some', 'their', 'for', 'why', 'through', 'again', 'own', 'to', 'with', 'so', 'but', 'i', 'more', 'hers', 'most', 'theirs', 'no', \"you'll\", 'after', 'any', 'wouldn', 'wasn', 'shan', 'during', \"couldn't\", 'mustn', 'and', 'while', 'same', 'too', \"don't\", 'whom', 'down', 'him', 'than', 'such', \"you'd\", 'each', 'all', 'yourselves', \"you've\", 'doing', 'having', 'yourself', 'does', \"wasn't\", \"hasn't\", 'was', 'its', 't', 'mightn', 'ours', 'other', 'then', 'itself', \"wouldn't\", 'when', 're', 'she', 'needn', 'which', 'there', \"she's\", \"hadn't\", 'you', 'because', 'if', 'am', 'that', 'under', 'into', 'be', 'ourselves', 's', 'were', \"didn't\", 'myself', 'those', 'been', 'by', \"aren't\", 'further', \"you're\", 'only', 'from', 'had', 'weren', \"needn't\", \"mightn't\", 'm', 'where', 'aren', 'nor', 'now', \"that'll\", \"doesn't\", 'himself', 'ain', 'about', 'until', 'don', 'them', 'the', 'won', 'before', 'ma', 'of', 'against'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "Stemmed Tokens : ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
      "Lemmatized Tokens : ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
      "['text', 'mine', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(\"Text mining is to identify useful information.\")\n",
    "print(tokens)\n",
    "\n",
    "tokens = stem(tokens)\n",
    "print(f\"Stemmed Tokens : {tokens}\")\n",
    "\n",
    "tokens = lemmatize(tokens)\n",
    "print(f\"Lemmatized Tokens : {tokens}\")\n",
    "\n",
    "\n",
    "print(filter_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single word is sometimes weakly expressive so that n-gram is a common method about better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens,n=1):\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        if len(tokens) != 1:\n",
    "            return [\" \".join(tokens[i:i+n]) for i in range(0,len(tokens)-n + 1)]\n",
    "        else:\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    When making the n-grams we pass in the stemmed/lematized words\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n"
     ]
    }
   ],
   "source": [
    "bi_gram = n_gram(tokens, 2)\n",
    "print(bi_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine is', 'mine is to', 'is to identifi', 'to identifi use', 'identifi use inform', 'use inform .']\n"
     ]
    }
   ],
   "source": [
    "tri_gram = n_gram(tokens, 3)\n",
    "print(tri_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bag-of-words representation\n",
    "\n",
    "Converting the words to numerical features.\n",
    "\n",
    "### One-hot vector\n",
    "\n",
    "a one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
    "\n",
    "For an example sentence `text mining is good`, we map all the words into indexes: map `text` to 0, `mining` to 1, `is` to 2, and `good` to 3.\n",
    "\n",
    "Then the one hot vector for `text` is `[1, 0, 0, 0]`. For `mining` it is `[0, 1, 0, 0]`, and so on.\n",
    "\n",
    "### Bag of word (BOW)\n",
    "\n",
    "The BOW vector of a **sentence** is the sum of all the one-hot vectors.\n",
    "\n",
    "For `text mining is good`, the BOW representation is `[1, 1, 1, 1]`.\n",
    "\n",
    "For `text mining good`, the BOW representation is `[1, 1, 0, 1]`.\n",
    "\n",
    "For `text mining is good mining`, the BOW representation is `[1, 2, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vocabulary_mapping(document_list):\n",
    "    \"\"\"\n",
    "    :param document_list: a list of sentences in token format: list\n",
    "    \n",
    "    return vocab_dict: a dict from words to indices, type: dict\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    for tokens in document_list:\n",
    "        for token in tokens:\n",
    "            if not token in vocab_dict:\n",
    "                vocab_dict[token] = len(vocab_dict)\n",
    "    return vocab_dict\n",
    "\n",
    "def get_bow_vector(document_list, vocab_dict):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokenized words, type: list\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    \n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(shape = (len(document_list),len(vocab_dict)), dtype=np.float)\n",
    "    for i,tokens in enumerate(document_list):\n",
    "        for f in tokens:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "            f_idx = vocab_dict.get(f, -1)\n",
    "            if f_idx != -1:\n",
    "                # set the corresponding element as 1\n",
    "                vector[i,f_idx] = vector[i,f_idx] + 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocabulary_mapping([[\"text\", \"mining\", \"is\", \"good\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 0, 'mining': 1, 'is': 2, 'good': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "tokens_1 = tokenize(\"text mining is good mining.\")\n",
    "tokens_2 = tokenize(\"text is good\")\n",
    "tokens_3 = tokenize(\"good text is good\")\n",
    "# print(get_bow_vector(tokens_1, vocab))\n",
    "# print(get_bow_vector(tokens_2, vocab))\n",
    "# print(get_bow_vector(tokens_3, vocab))\n",
    "print(get_bow_vector([tokens_1,tokens_2,tokens_3],vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1,2,3]\n",
    "A[0] = 2\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more elegant way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bow(token_list):\n",
    "    #make all the words into lower case\n",
    "    token_list = [list(map(lambda x:x.lower(),tokens)) for tokens in token_list]\n",
    "    #get the unique words in all the sentences in the list\n",
    "    unique_word_list = set([word for tokens in token_list for word in tokens])\n",
    "    #sort the words to have a consistent order\n",
    "    unique_word_list = sorted(unique_word_list)\n",
    "    bag_of_word_list = []\n",
    "    [bag_of_word_list.append([tokens.count(word) for word in unique_word_list]) for tokens in token_list]\n",
    "    return np.array(bag_of_word_list),unique_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus):\n",
    "    \"\"\"\n",
    "        :param corpus: a list of strings (type: list)\n",
    "    \"\"\"\n",
    "\n",
    "    #First tokenize each string in the corpus\n",
    "    processed_list = list()\n",
    "    for sentence in corpus:\n",
    "        processed_list.append(tokenize(sentence))\n",
    "    \n",
    "    #Next lematize and filter stopwords the words in the processed_list:\n",
    "    for i,tokens in enumerate(processed_list):\n",
    "        processed_list[i] = filter_stopwords(lemmatize(tokens))\n",
    "    #get bag of words\n",
    "    bag_of_words,unique_word_list = calculate_bow(processed_list)\n",
    "\n",
    "    return bag_of_words,unique_word_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_bow(corpus):\n",
    "#     \"\"\"\n",
    "#         :param corpus: a list of strings (type: list)\n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     def vectorize(sentence, vocab):\n",
    "#         \"\"\"\n",
    "#            :param sentence: a string (type:str)\n",
    "#            :param vocab: vocabulary (type:list) \n",
    "           \n",
    "#            count function of a list is to count the time of occurance in a list\n",
    "#            :return BOW vector\n",
    "#         \"\"\"\n",
    "#         return [sentence.split().count(i) for i in vocab]\n",
    "\n",
    "#     vectorized_corpus = []\n",
    "#     vocab = sorted(set([token for doc in corpus for token in doc.lower().split()]))\n",
    "#     for sent in corpus:\n",
    "#         vectorized_corpus.append((sent, vectorize(sent, vocab)))\n",
    "#     return vectorized_corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = [\"text mining is good mining\",\n",
    "\"text is good\",\n",
    "\"good text is good\"]\n",
    "corpus_bow,unique_word_list = process_corpus(all_sents)\n",
    "# corpus_bow, vocab = calculate_bow(all_sents)\n",
    "# print(corpus_bow)\n",
    "# print(vocab)\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity\n",
    "\n",
    "$$cos\\_sim(u, v) = \\frac{u \\cdot v}{|u||v|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def cosine_sim(u,v):\n",
    "    return np.dot(u,v) / (sqrt(np.dot(u,u)) * sqrt(np.dot(v,v)))\n",
    "\n",
    "def print_similarity(corpus):\n",
    "    \"\"\"\n",
    "    Print pairwise similarities\n",
    "    \"\"\"\n",
    "    for sentx in corpus:\n",
    "        for senty in corpus:\n",
    "            print(\"{:.4f}\".format(cosine_sim(sentx, senty)), end=\" \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000 0.5774 0.5477 \n",
      "0.5774 1.0000 0.9487 \n",
      "0.5477 0.9487 1.0000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similarity(corpus_bow)\n",
    "# [\"text mining is good mining\",\n",
    "# \"text is good\",\n",
    "# \"good text is good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Term weighting\n",
    "- Term frequency\n",
    "- Inverse document frequency\n",
    "- TF-IDF\n",
    "\n",
    "\n",
    "TF:\n",
    "\n",
    "$$\\text{tf}(t, d) = f_{t, d} \\bigg/ \\sum_{t'\\in d} f(t', d)$$\n",
    "\n",
    "Here, $f_{t, d}$ is the number of times term $t$ appearing in document $d$\n",
    "\n",
    "IDF:\n",
    "\n",
    "$$\\text{idf}(t) = N \\big/ n_t$$\n",
    "\n",
    "$N$ is the total number of docs in collection.\n",
    "\n",
    "$n_t$ is the number of docs containing term $t$\n",
    "\n",
    "Examples:\n",
    "```\n",
    "[\n",
    "doc 0: \"text mining is good mining\",\n",
    "doc 1: \"text is good\",\n",
    "doc 2: \"good text is good\"\n",
    "]\n",
    "```\n",
    "For doc 0, `\"good text is good\"`, `tf(\"text\", 0) = 1/4`\n",
    "\n",
    "The IDF of `\"text\"` is `3/3 = 1`\n",
    "\n",
    "The IDF of `\"mining\"` is `3/1 = 3`\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(freq_dict, term):\n",
    "    \"\"\"\n",
    "        :param freq_dict (type:dict): a dict variable whose key is the word and value is the frequency.\n",
    "            e.g., dict([\"text\":1, \"mining\":1, \"is\":1])\n",
    "        :param term (type:str): the candidate word to calculate TF\n",
    "        \n",
    "        :returns, the TF score of a certain word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tf = freq_dict[term] / float(sum(freq_dict.values()))\n",
    "        return tf\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def inverse_doc_freq(freq_dict_list, term):\n",
    "    \"\"\"\n",
    "        :param freq_dict_list (type: list): a list of freq_dict \n",
    "        :param term (type:str): the candidate word to calculate TF\n",
    "        \n",
    "        :returns, the IDF of a certain word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        unique_docs = sum([1 for i,_ in enumerate(freq_dict_list) if freq_dict_list[i][term] > 0])\n",
    "        return float(len(freq_dict_list)) / unique_docs\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow representation: [2 0 1]\n",
      "frequency dict: {'good': 2, 'mining': 0, 'text': 1}\n"
     ]
    }
   ],
   "source": [
    "# sentence: good text is good\n",
    "current_bow = corpus_bow[2]\n",
    "print(\"bow representation:\", current_bow)\n",
    "doc_vec_dict = {k:v for k,v in zip(unique_word_list, current_bow)}\n",
    "print(\"frequency dict:\", doc_vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: good 0.6666666666666666\n",
      "TF: text 0.3333333333333333\n",
      "TF: is 0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF: good\", term_freq(doc_vec_dict, \"good\"))\n",
    "print(\"TF: text\", term_freq(doc_vec_dict, \"text\"))\n",
    "print(\"TF: is\", term_freq(doc_vec_dict, \"is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'good': 1, 'mining': 2, 'text': 1}, {'good': 1, 'mining': 0, 'text': 1}, {'good': 2, 'mining': 0, 'text': 1}]\n"
     ]
    }
   ],
   "source": [
    "freq_dict_list = [{k:v for k,v in zip(unique_word_list, vecs)} for vecs in corpus_bow]\n",
    "print(freq_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': 1, 'mining': 2, 'text': 1}\n",
      "{'good': 1, 'mining': 0, 'text': 1}\n",
      "{'good': 2, 'mining': 0, 'text': 1}\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(freq_dict_list):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: good 1.0\n",
      "IDF: text 1.0\n",
      "IDF: is 1.0\n",
      "IDF: mining 3.0\n"
     ]
    }
   ],
   "source": [
    "# all_sents = [\n",
    "# \"text mining is good mining\",\n",
    "# \"text is good\",\n",
    "# \"good text is good\"]\n",
    "print(\"IDF: good\", inverse_doc_freq(freq_dict_list, \"good\"))\n",
    "print(\"IDF: text\", inverse_doc_freq(freq_dict_list, \"text\"))\n",
    "print(\"IDF: is\", inverse_doc_freq(freq_dict_list, \"is\"))\n",
    "print(\"IDF: mining\", inverse_doc_freq(freq_dict_list, \"mining\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_PS(corpus_bow,unique_word_list):\n",
    "    \"\"\"\n",
    "        :params corpus_bow : a numpy array of the frequency of each word\n",
    "        :params unique_word_list : a list of unique words\n",
    "    \"\"\"\n",
    "    #obtain word positioning \n",
    "    word_position = {k:v for k,v in zip(unique_word_list,range(len(unique_word_list)))}\n",
    "\n",
    "    tf_idf_vector = np.zeros(shape = (corpus_bow.shape[0],len(unique_word_list)))\n",
    "    #creating the frequency dictionaries for each document\n",
    "    freq_dict_list = [dict(zip(unique_word_list,freq_array)) for freq_array in corpus_bow]\n",
    "\n",
    "    for i,freq_dict in enumerate(freq_dict_list):\n",
    "        for term in freq_dict:\n",
    "            tf = term_freq(freq_dict,term)\n",
    "            idf = inverse_doc_freq(freq_dict_list,term)\n",
    "            tf_idf_vector[i,word_position[term]] = tf*idf\n",
    "\n",
    "    return tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus_bow, vocab):\n",
    "    \"\"\"\n",
    "        :params corpus_bow(type: list): the BOW representation of the corpus\n",
    "        :params vocab (type:list): the list of vocab\n",
    "        \n",
    "        return the tf idf representation of the corpus\n",
    "    \"\"\"\n",
    "    word2id = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    freq_dict_list = [{k:v for k,v in zip(vocab, i)} for i in corpus_bow]\n",
    "    tfidf_mat  =  np.zeros((len(freq_dict_list), len(vocab)), dtype=float)\n",
    "    for doc_id, doc in enumerate(freq_dict_list):\n",
    "        for term in doc:\n",
    "            term_id = word2id[term]\n",
    "            tf = term_freq(freq_dict_list[doc_id],term)\n",
    "            idf = inverse_doc_freq(freq_dict_list, term)\n",
    "            tfidf_mat[doc_id][term_id] = tf*idf\n",
    "\n",
    "    all_sents = [doc[0] for doc in corpus_bow]\n",
    "    corpus_tfidf = list(zip(all_sents, tfidf_mat))\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 1.5       , 0.25      ],\n",
       "       [0.5       , 0.        , 0.5       ],\n",
       "       [0.66666667, 0.        , 0.33333333]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_tfidf_PS(corpus_bow,unique_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow_tfidf = calculate_tfidf(corpus_bow, unique_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1, 2, 1]), (1, array([0.25, 1.5 , 0.25]))),\n",
       " (array([1, 0, 1]), (1, array([0.5, 0. , 0.5]))),\n",
       " (array([2, 0, 1]), (2, array([0.66666667, 0.        , 0.33333333])))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(corpus_bow, corpus_bow_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'is', 'mining', 'text']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff0d91300876931207232d01add3156fa7c8214350996c757a3c6cebc4b3b5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
